{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"A high level framework and library for running, training, and deploying state-of-the-art Natural Language Processing (NLP) models for end to end tasks. AdaptNLP allows users ranging from beginner python coders to experienced machine learning engineers to leverage state-of-the-art NLP models and training techniques in one easy-to-use python package. Built atop Zalando Research's Flair and Hugging Face's Transformers library, AdaptNLP provides Machine Learning Researchers and Scientists a modular and adaptive approach to a variety of NLP tasks with an Easy API for training, inference, and deploying NLP-based microservices. Key Features Full Guides and API Documentation Tutorial Jupyter/Google Colab Notebooks Unified API for NLP Tasks with SOTA Pretrained Models (Adaptable with Flair and Transformer's Models) Token Tagging Sequence Classification Embeddings Question Answering Summarization Translation More in development Training and Fine-tuning Interface Jeremy's ULM-FIT approach for transfer learning in NLP Fine-tuning Transformer's language models and task-specific predictive heads like Flair's SequenceClassifier Rapid NLP Model Deployment with Sebasti\u00e1n's FastAPI Framework Containerized FastAPI app Immediately deploy any custom trained Flair or AdaptNLP model Dockerizing AdaptNLP with GPUs Easily build and run AdaptNLP containers leveraging NVIDIA GPUs with Docker Quick Start \u00b6 Requirements and Installation \u00b6 Virtual Environment \u00b6 To avoid dependency clustering and issues, it would be wise to install AdaptNLP in a virtual environment. To create a new python 3.6+ virtual environment, run this command and then activate it however your operating system specifies: python -m venv venv-adaptnlp AdaptNLP Install \u00b6 Install using pip in your virtual environment: pip install adaptnlp Examples and General Use \u00b6 Once you have installed AdaptNLP, here are a few examples of what you can run with AdaptNLP modules: Named Entity Recognition with EasyTokenTagger \u00b6 from adaptnlp import EasyTokenTagger ## Example Text example_text = \"Novetta's headquarters is located in Mclean, Virginia.\" ## Load the token tagger module and tag text with the NER model tagger = EasyTokenTagger () sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner\" ) ## Output tagged token span results in Flair's Sentence object model for sentence in sentences : for entity in sentence . get_spans ( \"ner\" ): print ( entity ) English Sentiment Classifier EasySequenceClassifier \u00b6 from adaptnlp import EasySequenceClassifier ## Example Text example_text = \"Novetta is a great company that was chosen as one of top 50 great places to work!\" ## Load the sequence classifier module and classify sequence of text with the english sentiment model classifier = EasySequenceClassifier () sentences = classifier . tag_text ( text = example_text , mini_batch_size = 1 , model_name_or_path = \"en-sentiment\" ) ## Output labeled text results in Flair's Sentence object model for sentence in sentences : print ( sentence . labels ) Span-based Question Answering EasyQuestionAnswering \u00b6 from adaptnlp import EasyQuestionAnswering ## Example Query and Context query = \"What is the meaning of life?\" context = \"Machine Learning is the meaning of life.\" top_n = 5 ## Load the QA module and run inference on results qa = EasyQuestionAnswering () best_answer , best_n_answers = qa . predict_qa ( query = query , context = context , n_best_size = top_n , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) ## Output top answer as well as top 5 answers print ( best_answer ) print ( best_n_answers ) Sequence Classification Training SequenceClassifier \u00b6 from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer # Specify corpus data directory and model output directory corpus = \"Path/to/data/directory\" OUTPUT_DIR = \"Path/to/output/directory\" # Instantiate AdaptNLP easy document embeddings module, which can take in a variable number of embeddings to make `Stacked Embeddings`. # You may also use custom Transformers LM models by specifying the path the the language model doc_embeddings = EasyDocumentEmbeddings ( model_name_or_path = \"bert-base-cased\" , methods = [ \"rnn\" ]) # Instantiate Sequence Classifier Trainer by loading in the data, data column map, and embeddings as an encoder sc_trainer = SequenceClassifierTrainer ( corpus = corpus , encoder = doc_embeddings , column_name_map = { 0 : \"text\" , 1 : \"label\" }) # Find Learning Rate learning_rate = sc_trainer . find_learning_rate ( output_dir = OUTPUT_DIR ) # Train Using Flair's Sequence Classification Head sc_trainer . train ( output_dir = OUTPUT_DIR , learning_rate = learning_rate , max_epochs = 150 ) # Predict text labels with the trained model using `EasySequenceClassifier` from adaptnlp import EasySequenceClassifier example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR / \"final-model.pt\" ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels ) Transformers Language Model Fine Tuning LMFineTuner \u00b6 from adaptnlp import LMFineTuner # Specify Text Data File Paths train_data_file = \"Path/to/train.csv\" eval_data_file = \"Path/to/test.csv\" # Instantiate Finetuner with Desired Language Model finetuner = LMFineTuner ( train_data_file = train_data_file , eval_data_file = eval_data_file , model_type = \"bert\" , model_name_or_path = \"bert-base-cased\" ) finetuner . freeze () # Find Optimal Learning Rate learning_rate = finetuner . find_learning_rate ( base_path = \"Path/to/base/directory\" ) finetuner . freeze () # Train and Save Fine Tuned Language Models finetuner . train_one_cycle ( output_dir = \"Path/to/output/directory\" , learning_rate = learning_rate ) Tutorials \u00b6 Look in the Tutorials directory for a quick introduction to the library and its very simple and straight forward use cases: NLP Tasks 1. Token Classification: NER, POS, Chunk, and Frame Tagging - 2. Sequence Classification: Sentiment - 3. Embeddings: Transformer Embeddings e.g. BERT, XLM, GPT2, XLNet, roBERTa, ALBERT - 4. Question Answering: Span-based Question Answering Model - 5. Summarization: Abstractive and Extractive - 6. Translation: Seq2Seq - Custom Fine-Tuning and Training with Transformer Models - Training a Sequence Classifier - - Fine-tuning a Transformers Language Model - Checkout the documentation for more information. REST Service \u00b6 We use FastAPI for standing up endpoints for serving state-of-the-art NLP models with AdaptNLP. The REST directory contains more detail on deploying a REST API locally or with docker in a very easy and fast way. Docker \u00b6 AdaptNLP official docker images are up on Docker Hub . Images have AdaptNLP installed from source in developer mode with tutorial notebooks available. Images can build with GPU support if NVIDA-Docker is correctly installed. Pull and Run AdaptNLP Immediately \u00b6 Simply run an image with AdaptNLP installed from source in developer mode by running: docker run -it --rm achangnovetta/adaptnlp:latest Run an image with AdaptNLP running on GPUs if you have nvidia drivers and nvidia-docker 19.03+ installed: docker run -it --rm --gpus all achangnovetta/adaptnlp:latest Build \u00b6 Build docker image and run container with the following commands in the directory of the Dockerfile to create a container with adaptnlp installed and ready to go Note: A container with GPUs enabled requires Docker version 19.03+ and nvida-docker installed docker build -t achangnovetta/adaptnlp:latest . docker run -it --rm achangnovetta/adaptnlp:latest If you want to use CUDA compatible GPUs docker run -it --rm --gpus all achangnovetta/adaptnlp:latest Contact \u00b6 Please contact the author Andrew Chang at achang@novetta.com with questions or comments regarding AdaptNLP. Follow us on Twitter at @achang1618 and @AdaptNLP for updates and NLP dialogue. License \u00b6 This project is licensed under the terms of the Apache 2.0 license.","title":"AdaptNLP"},{"location":"index.html#quick-start","text":"","title":"Quick Start"},{"location":"index.html#requirements-and-installation","text":"","title":"Requirements and Installation"},{"location":"index.html#virtual-environment","text":"To avoid dependency clustering and issues, it would be wise to install AdaptNLP in a virtual environment. To create a new python 3.6+ virtual environment, run this command and then activate it however your operating system specifies: python -m venv venv-adaptnlp","title":"Virtual Environment"},{"location":"index.html#adaptnlp-install","text":"Install using pip in your virtual environment: pip install adaptnlp","title":"AdaptNLP Install"},{"location":"index.html#examples-and-general-use","text":"Once you have installed AdaptNLP, here are a few examples of what you can run with AdaptNLP modules:","title":"Examples and General Use"},{"location":"index.html#named-entity-recognition-with-easytokentagger","text":"from adaptnlp import EasyTokenTagger ## Example Text example_text = \"Novetta's headquarters is located in Mclean, Virginia.\" ## Load the token tagger module and tag text with the NER model tagger = EasyTokenTagger () sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner\" ) ## Output tagged token span results in Flair's Sentence object model for sentence in sentences : for entity in sentence . get_spans ( \"ner\" ): print ( entity )","title":"Named Entity Recognition with EasyTokenTagger"},{"location":"index.html#english-sentiment-classifier-easysequenceclassifier","text":"from adaptnlp import EasySequenceClassifier ## Example Text example_text = \"Novetta is a great company that was chosen as one of top 50 great places to work!\" ## Load the sequence classifier module and classify sequence of text with the english sentiment model classifier = EasySequenceClassifier () sentences = classifier . tag_text ( text = example_text , mini_batch_size = 1 , model_name_or_path = \"en-sentiment\" ) ## Output labeled text results in Flair's Sentence object model for sentence in sentences : print ( sentence . labels )","title":"English Sentiment Classifier EasySequenceClassifier"},{"location":"index.html#span-based-question-answering-easyquestionanswering","text":"from adaptnlp import EasyQuestionAnswering ## Example Query and Context query = \"What is the meaning of life?\" context = \"Machine Learning is the meaning of life.\" top_n = 5 ## Load the QA module and run inference on results qa = EasyQuestionAnswering () best_answer , best_n_answers = qa . predict_qa ( query = query , context = context , n_best_size = top_n , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) ## Output top answer as well as top 5 answers print ( best_answer ) print ( best_n_answers )","title":"Span-based Question Answering EasyQuestionAnswering"},{"location":"index.html#sequence-classification-training-sequenceclassifier","text":"from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer # Specify corpus data directory and model output directory corpus = \"Path/to/data/directory\" OUTPUT_DIR = \"Path/to/output/directory\" # Instantiate AdaptNLP easy document embeddings module, which can take in a variable number of embeddings to make `Stacked Embeddings`. # You may also use custom Transformers LM models by specifying the path the the language model doc_embeddings = EasyDocumentEmbeddings ( model_name_or_path = \"bert-base-cased\" , methods = [ \"rnn\" ]) # Instantiate Sequence Classifier Trainer by loading in the data, data column map, and embeddings as an encoder sc_trainer = SequenceClassifierTrainer ( corpus = corpus , encoder = doc_embeddings , column_name_map = { 0 : \"text\" , 1 : \"label\" }) # Find Learning Rate learning_rate = sc_trainer . find_learning_rate ( output_dir = OUTPUT_DIR ) # Train Using Flair's Sequence Classification Head sc_trainer . train ( output_dir = OUTPUT_DIR , learning_rate = learning_rate , max_epochs = 150 ) # Predict text labels with the trained model using `EasySequenceClassifier` from adaptnlp import EasySequenceClassifier example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR / \"final-model.pt\" ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels )","title":"Sequence Classification Training SequenceClassifier"},{"location":"index.html#transformers-language-model-fine-tuning-lmfinetuner","text":"from adaptnlp import LMFineTuner # Specify Text Data File Paths train_data_file = \"Path/to/train.csv\" eval_data_file = \"Path/to/test.csv\" # Instantiate Finetuner with Desired Language Model finetuner = LMFineTuner ( train_data_file = train_data_file , eval_data_file = eval_data_file , model_type = \"bert\" , model_name_or_path = \"bert-base-cased\" ) finetuner . freeze () # Find Optimal Learning Rate learning_rate = finetuner . find_learning_rate ( base_path = \"Path/to/base/directory\" ) finetuner . freeze () # Train and Save Fine Tuned Language Models finetuner . train_one_cycle ( output_dir = \"Path/to/output/directory\" , learning_rate = learning_rate )","title":"Transformers Language Model Fine Tuning LMFineTuner"},{"location":"index.html#tutorials","text":"Look in the Tutorials directory for a quick introduction to the library and its very simple and straight forward use cases: NLP Tasks 1. Token Classification: NER, POS, Chunk, and Frame Tagging - 2. Sequence Classification: Sentiment - 3. Embeddings: Transformer Embeddings e.g. BERT, XLM, GPT2, XLNet, roBERTa, ALBERT - 4. Question Answering: Span-based Question Answering Model - 5. Summarization: Abstractive and Extractive - 6. Translation: Seq2Seq - Custom Fine-Tuning and Training with Transformer Models - Training a Sequence Classifier - - Fine-tuning a Transformers Language Model - Checkout the documentation for more information.","title":"Tutorials"},{"location":"index.html#rest-service","text":"We use FastAPI for standing up endpoints for serving state-of-the-art NLP models with AdaptNLP. The REST directory contains more detail on deploying a REST API locally or with docker in a very easy and fast way.","title":"REST Service"},{"location":"index.html#docker","text":"AdaptNLP official docker images are up on Docker Hub . Images have AdaptNLP installed from source in developer mode with tutorial notebooks available. Images can build with GPU support if NVIDA-Docker is correctly installed.","title":"Docker"},{"location":"index.html#pull-and-run-adaptnlp-immediately","text":"Simply run an image with AdaptNLP installed from source in developer mode by running: docker run -it --rm achangnovetta/adaptnlp:latest Run an image with AdaptNLP running on GPUs if you have nvidia drivers and nvidia-docker 19.03+ installed: docker run -it --rm --gpus all achangnovetta/adaptnlp:latest","title":"Pull and Run AdaptNLP Immediately"},{"location":"index.html#build","text":"Build docker image and run container with the following commands in the directory of the Dockerfile to create a container with adaptnlp installed and ready to go Note: A container with GPUs enabled requires Docker version 19.03+ and nvida-docker installed docker build -t achangnovetta/adaptnlp:latest . docker run -it --rm achangnovetta/adaptnlp:latest If you want to use CUDA compatible GPUs docker run -it --rm --gpus all achangnovetta/adaptnlp:latest","title":"Build"},{"location":"index.html#contact","text":"Please contact the author Andrew Chang at achang@novetta.com with questions or comments regarding AdaptNLP. Follow us on Twitter at @achang1618 and @AdaptNLP for updates and NLP dialogue.","title":"Contact"},{"location":"index.html#license","text":"This project is licensed under the terms of the Apache 2.0 license.","title":"License"},{"location":"contributing.html","text":"Contributing for AdaptNLP \u00b6 Contributions are welcome for AdaptNLP via. PRs. Make sure all features, bugs, etc. changes are addressed with an issue number. If no issue posts regarding the PR are found, please create one before submitting the PR. Early contributions for v0.1.* may be limited due to rapid development and potential design changes.","title":"Contributing"},{"location":"contributing.html#contributing-for-adaptnlp","text":"Contributions are welcome for AdaptNLP via. PRs. Make sure all features, bugs, etc. changes are addressed with an issue number. If no issue posts regarding the PR are found, please create one before submitting the PR. Early contributions for v0.1.* may be limited due to rapid development and potential design changes.","title":"Contributing for AdaptNLP"},{"location":"rest.html","text":"AdaptNLP-Rest API \u00b6 Getting Started \u00b6 Docker \u00b6 The docker image of AdaptNLP is built with the achangnovetta/adaptnlp:latest image. To build and run the rest services by running one of the following methods in this directory: 1. Docker Build Env Arg Entries \u00b6 Specify the pretrained models you want to use for the endpoints. This can be one of Flair's pretrained models or your own custom trained models with a path pointing to the model. (The model must be in this directory) docker build -t adaptnlp-rest:latest --build-arg TOKEN_TAGGING_MODE=ner \\ --build-arg TOKEN_TAGGING_MODEL=ner-ontonotes-fast \\ --build-arg SEQUENCE_CLASSIFICATION_MODEL=en-sentiment . docker run -itp 5000:5000 adaptnlp-rest:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all adaptnlp-rest:latest bash 2. Docker Run Env Arg Entries \u00b6 If you'd like to specify the models as environment variables in docker post-build, run the below instead: docker build -t adaptnlp-rest:latest . docker run -itp 5000:5000 -e TOKEN_TAGGING_MODE='ner' \\ -e TOKEN_TAGGING_MODEL='ner-ontonotes-fast' \\ -e SEQUENCE_CLASSIFICATION_MODEL='en-sentiment' \\ adaptnlp-rest:latest \\ bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all -e TOKEN_TAGGING_MODE='ner' \\ -e TOKEN_TAGGING_MODEL='ner-ontonotes-fast' \\ -e SEQUENCE_CLASSIFICATION_MODEL='en-sentiment' \\ adaptnlp-rest:latest \\ bash Manual \u00b6 If you just want to run the rest services locally in an environment that has AdaptNLP installed, you can run the following in this directory: pip install -r requirements export TOKEN_TAGGING_MODE=ner export TOKEN_TAGGING_MODEL=ner-ontonotes-fast export SEQUENCE_CLASSIFICATION_MODEL=en-sentiment uvicorn app.main:app --host 0.0.0.0 --port 5000 SwaggerUI \u00b6 Access SwaggerUI console by going to localhost:5000/docs after deploying","title":"NLP Services with FastAPI"},{"location":"rest.html#adaptnlp-rest-api","text":"","title":"AdaptNLP-Rest API"},{"location":"rest.html#getting-started","text":"","title":"Getting Started"},{"location":"rest.html#docker","text":"The docker image of AdaptNLP is built with the achangnovetta/adaptnlp:latest image. To build and run the rest services by running one of the following methods in this directory:","title":"Docker"},{"location":"rest.html#1-docker-build-env-arg-entries","text":"Specify the pretrained models you want to use for the endpoints. This can be one of Flair's pretrained models or your own custom trained models with a path pointing to the model. (The model must be in this directory) docker build -t adaptnlp-rest:latest --build-arg TOKEN_TAGGING_MODE=ner \\ --build-arg TOKEN_TAGGING_MODEL=ner-ontonotes-fast \\ --build-arg SEQUENCE_CLASSIFICATION_MODEL=en-sentiment . docker run -itp 5000:5000 adaptnlp-rest:latest bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all adaptnlp-rest:latest bash","title":"1. Docker Build Env Arg Entries"},{"location":"rest.html#2-docker-run-env-arg-entries","text":"If you'd like to specify the models as environment variables in docker post-build, run the below instead: docker build -t adaptnlp-rest:latest . docker run -itp 5000:5000 -e TOKEN_TAGGING_MODE='ner' \\ -e TOKEN_TAGGING_MODEL='ner-ontonotes-fast' \\ -e SEQUENCE_CLASSIFICATION_MODEL='en-sentiment' \\ adaptnlp-rest:latest \\ bash To run with GPUs if you have nvidia-docker installed with with compatible NVIDIA drivers docker run -itp 5000:5000 --gpus all -e TOKEN_TAGGING_MODE='ner' \\ -e TOKEN_TAGGING_MODEL='ner-ontonotes-fast' \\ -e SEQUENCE_CLASSIFICATION_MODEL='en-sentiment' \\ adaptnlp-rest:latest \\ bash","title":"2. Docker Run Env Arg Entries"},{"location":"rest.html#manual","text":"If you just want to run the rest services locally in an environment that has AdaptNLP installed, you can run the following in this directory: pip install -r requirements export TOKEN_TAGGING_MODE=ner export TOKEN_TAGGING_MODEL=ner-ontonotes-fast export SEQUENCE_CLASSIFICATION_MODEL=en-sentiment uvicorn app.main:app --host 0.0.0.0 --port 5000","title":"Manual"},{"location":"rest.html#swaggerui","text":"Access SwaggerUI console by going to localhost:5000/docs after deploying","title":"SwaggerUI"},{"location":"class-api/embeddings-module.html","text":"EasyWordEmbeddings \u00b6 class adaptnlp. EasyWordEmbeddings ( ) Word embeddings from the latest language models Usage: >>> embeddings = adaptnlp . EasyWordEmbeddings () >>> embeddings . embed_text ( \"text you want embeddings for\" , model_name_or_path = \"bert-base-cased\" ) embed_all ( self , text , *model_names_or_paths ) Embeds text with all embedding models loaded text - Text input, it can be a string or any of Flair's Sentence input formats model_names_or_paths - A variable input of model names or paths to embed return - A list of Flair's Sentence s embed_text ( self , text , model_name_or_path='bert-base-cased' ) Produces embeddings for text text - Text input, it can be a string or any of Flair's Sentence input formats model_name_or_path - The hosted model name key or model path return - A list of Flair's Sentence s EasyStackedEmbeddings \u00b6 class adaptnlp. EasyStackedEmbeddings ( *embeddings ) Word Embeddings that have been concatenated and \"stacked\" as specified by flair Usage: >>> embeddings = adaptnlp . EasyStackedEmbeddings ( \"bert-base-cased\" , \"gpt2\" , \"xlnet-base-cased\" ) Parameters: *embeddings - Non-keyword variable number of strings specifying the embeddings you want to stack embed_text ( self , text ) Stacked embeddings text - Text input, it can be a string or any of Flair's Sentence input formats return A list of Flair's Sentence s EasyDocumentEmbeddings \u00b6 class adaptnlp. EasyDocumentEmbeddings ( *embeddings , methods=['rnn', 'pool'] , configs={'pool_configs': {'fine_tune_mode': 'linear', 'pooling': 'mean'}, 'rnn_configs': {'hidden_size': 512, 'rnn_layers': 1, 'reproject_words': True, 'reproject_words_dimension': 256, 'bidirectional': False, 'dropout': 0.5, 'word_dropout': 0.0, 'locked_dropout': 0.0, 'rnn_type': 'GRU', 'fine_tune': True}} ) Document Embeddings generated by pool and rnn methods applied to the word embeddings of text Usage: >>> embeddings = adaptnlp . EasyDocumentEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" , methods [ \"rnn\" ]) Parameters: *embeddings - Non-keyword variable number of strings referring to model names or paths methods - A list of strings to specify which document embeddings to use i.e. [\"rnn\", \"pool\"] (avoids unncessary loading of models if only using one) configs - A dictionary of configurations for flair's rnn and pool document embeddings >>> example_configs = { \"pool_configs\" : { \"fine_tune_mode\" : \"linear\" , \"pooling\" : \"mean\" , }, ... \"rnn_configs\" : { \"hidden_size\" : 512 , ... \"rnn_layers\" : 1 , ... \"reproject_words\" : True , ... \"reproject_words_dimension\" : 256 , ... \"bidirectional\" : False , ... \"dropout\" : 0.5 , ... \"word_dropout\" : 0.0 , ... \"locked_dropout\" : 0.0 , ... \"rnn_type\" : \"GRU\" , ... \"fine_tune\" : True , }, ... } embed_pool ( self , text ) Stacked embeddings text - Text input, it can be a string or any of Flair's Sentence input formats return - A list of Flair's Sentence s embed_rnn ( self , text ) Stacked embeddings text - Text input, it can be a string or any of Flair's Sentence input formats return - A list of Flair's Sentence s","title":"Embeddings"},{"location":"class-api/embeddings-module.html#easywordembeddings","text":"class adaptnlp. EasyWordEmbeddings ( ) Word embeddings from the latest language models Usage: >>> embeddings = adaptnlp . EasyWordEmbeddings () >>> embeddings . embed_text ( \"text you want embeddings for\" , model_name_or_path = \"bert-base-cased\" ) embed_all ( self , text , *model_names_or_paths ) Embeds text with all embedding models loaded text - Text input, it can be a string or any of Flair's Sentence input formats model_names_or_paths - A variable input of model names or paths to embed return - A list of Flair's Sentence s embed_text ( self , text , model_name_or_path='bert-base-cased' ) Produces embeddings for text text - Text input, it can be a string or any of Flair's Sentence input formats model_name_or_path - The hosted model name key or model path return - A list of Flair's Sentence s","title":"EasyWordEmbeddings"},{"location":"class-api/embeddings-module.html#easystackedembeddings","text":"class adaptnlp. EasyStackedEmbeddings ( *embeddings ) Word Embeddings that have been concatenated and \"stacked\" as specified by flair Usage: >>> embeddings = adaptnlp . EasyStackedEmbeddings ( \"bert-base-cased\" , \"gpt2\" , \"xlnet-base-cased\" ) Parameters: *embeddings - Non-keyword variable number of strings specifying the embeddings you want to stack embed_text ( self , text ) Stacked embeddings text - Text input, it can be a string or any of Flair's Sentence input formats return A list of Flair's Sentence s","title":"EasyStackedEmbeddings"},{"location":"class-api/embeddings-module.html#easydocumentembeddings","text":"class adaptnlp. EasyDocumentEmbeddings ( *embeddings , methods=['rnn', 'pool'] , configs={'pool_configs': {'fine_tune_mode': 'linear', 'pooling': 'mean'}, 'rnn_configs': {'hidden_size': 512, 'rnn_layers': 1, 'reproject_words': True, 'reproject_words_dimension': 256, 'bidirectional': False, 'dropout': 0.5, 'word_dropout': 0.0, 'locked_dropout': 0.0, 'rnn_type': 'GRU', 'fine_tune': True}} ) Document Embeddings generated by pool and rnn methods applied to the word embeddings of text Usage: >>> embeddings = adaptnlp . EasyDocumentEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" , methods [ \"rnn\" ]) Parameters: *embeddings - Non-keyword variable number of strings referring to model names or paths methods - A list of strings to specify which document embeddings to use i.e. [\"rnn\", \"pool\"] (avoids unncessary loading of models if only using one) configs - A dictionary of configurations for flair's rnn and pool document embeddings >>> example_configs = { \"pool_configs\" : { \"fine_tune_mode\" : \"linear\" , \"pooling\" : \"mean\" , }, ... \"rnn_configs\" : { \"hidden_size\" : 512 , ... \"rnn_layers\" : 1 , ... \"reproject_words\" : True , ... \"reproject_words_dimension\" : 256 , ... \"bidirectional\" : False , ... \"dropout\" : 0.5 , ... \"word_dropout\" : 0.0 , ... \"locked_dropout\" : 0.0 , ... \"rnn_type\" : \"GRU\" , ... \"fine_tune\" : True , }, ... } embed_pool ( self , text ) Stacked embeddings text - Text input, it can be a string or any of Flair's Sentence input formats return - A list of Flair's Sentence s embed_rnn ( self , text ) Stacked embeddings text - Text input, it can be a string or any of Flair's Sentence input formats return - A list of Flair's Sentence s","title":"EasyDocumentEmbeddings"},{"location":"class-api/language-model-finetuner-module.html","text":"LMFineTuner \u00b6 class adaptnlp. LMFineTuner ( train_data_file , eval_data_file=None , model_type='bert' , model_name_or_path=None , mlm=True , mlm_probability=0.15 , config_name=None , tokenizer_name=None , cache_dir=None , block_size=-1 , no_cuda=False , overwrite_cache=False , seed=42 , fp16=False , fp16_opt_level='O1' , local_rank=-1 ) A Language Model Fine Tuner object you can set language model configurations and then train and evaluate Usage: >>> finetuner = adaptnlp . LMFineTuner () >>> finetuner . train () Parameters: train_data_file - The input training data file (a text file). eval_data_file - An optional input evaluation data file to evaluate the perplexity on (a text file). model_type - The model architecture to be trained or fine-tuned. model_name_or_path - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch. mlm - Train with masked-language modeling loss instead of language modeling. mlm_probability - Ratio of tokens to mask for masked language modeling loss config_name - Optional Transformers pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config. tokenizer_name - Optional Transformers pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer. cache_dir - Optional directory to store the pre-trained models downloaded from s3 (If None, will go to default dir) block_size - Optional input sequence length after tokenization. The training dataset will be truncated in block of this size for training.\" -1 will default to the model max input length for single sentence inputs (take into account special tokens). no_cuda - Avoid using CUDA when available overwrite_cache - Overwrite the cached training and evaluation sets seed - random seed for initialization fp16 - Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit fp16_opt_level - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. local_rank - For distributed training: local_rank find_learning_rate ( self , output_dir , file_name='learning_rate.tsv' , start_learning_rate=1e-07 , end_learning_rate=10 , iterations=100 , mini_batch_size=8 , stop_early=True , smoothing_factor=0.7 , adam_epsilon=1e-08 , weight_decay=0.0 , **kwargs ) Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot This method returns a suggested learning rate using the static method LMFineTuner.suggest_learning_rate() which is implicitly run in this method. output_dir - Path to dir for learning rate file to be saved file_name - Name of learning rate .tsv file start_learning_rate - Initial learning rate to start cyclical learning rate finder method end_learning_rate - End learning rate to stop exponential increase of the learning rate iterations - Number of optimizer iterations for the ExpAnnealLR scheduler mini_batch_size - Batch size for dataloader stop_early - Bool for stopping early once loss diverges smoothing_factor - Smoothing factor on moving average of losses adam_epsilon - Epsilon for Adam optimizer. weight_decay - Weight decay if we apply some. kwargs - Additional keyword arguments for the Adam optimizer return - Learning rate as a float suggest_learning_rate ( losses , lrs , lr_diff=30 , loss_threshold=0.05 , adjust_value=1 ) Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method losses - Numpy array of losses lrs - Numpy array of exponentially increasing learning rates (must match dim of losses ) lr_diff - Learning rate Interval of slide ruler loss_threshold - Threshold of loss difference on interval where the sliding stops adjust_value - Coefficient for adjustment return - the optimal learning rate as a float train_one_cycle ( self , output_dir , should_continue=False , overwrite_output_dir=False , evaluate_during_training=False , per_gpu_train_batch_size=4 , gradient_accumulation_steps=1 , learning_rate=5e-05 , weight_decay=0.0 , adam_epsilon=1e-08 , max_grad_norm=1.0 , num_train_epochs=1.0 , max_steps=-1 , warmup_steps=0 , logging_steps=50 , save_steps=50 , save_total_limit=3 , use_tensorboard=False ) output_dir - The output directory where the model predictions and checkpoints will be written. should_continue - Whether to continue training from latest checkpoint in output_dir overwrite_output_dir - Overwrite the content of output directory output_dir evaluate_during_training - Run evaluation during training at each logging_step . per_gpu_train_batch_size - Batch size per GPU/CPU for training. (If evaluate_during_training is True, this is also the eval batch size gradient_accumulation_steps - Number of updates steps to accumulate before performing a backward/update pass learning_rate - The initial learning rate for Adam optimizer. weight_decay - Weight decay if we apply some. adam_epsilon - Epsilon for Adam optimizer. max_grad_norm - Max gradient norm. Duh num_train_epochs - Total number of training epochs to perform. max_steps - If > 0: set total number of training steps to perform. Override num_train_epochs . warmup_steps - Linear warmup over warmup_steps. logging_steps - Number of steps until logging occurs. save_steps - Number of steps until checkpoint is saved in output_dir save_total_limit - Limit the total amount of checkpoints, delete the older checkpoints in the output_dir , does not delete by default use_tensorboard - Only useable if tensorboard is installed return - None train ( self , output_dir , should_continue=False , overwrite_output_dir=False , evaluate_during_training=False , per_gpu_train_batch_size=4 , gradient_accumulation_steps=1 , learning_rate=5e-05 , weight_decay=0.0 , adam_epsilon=1e-08 , max_grad_norm=1.0 , num_train_epochs=1.0 , max_steps=-1 , warmup_steps=0 , logging_steps=50 , save_steps=50 , save_total_limit=3 , use_tensorboard=False ) output_dir - The output directory where the model predictions and checkpoints will be written. should_continue - Whether to continue training from latest checkpoint in output_dir overwrite_output_dir - Overwrite the content of output directory output_dir evaluate_during_training - Run evaluation during training at each logging_step . per_gpu_train_batch_size - Batch size per GPU/CPU for training. (If evaluate_during_training is True, this is also the eval batch size gradient_accumulation_steps - Number of updates steps to accumulate before performing a backward/update pass learning_rate - The initial learning rate for Adam optimizer. weight_decay - Weight decay if we apply some. adam_epsilon - Epsilon for Adam optimizer. max_grad_norm - Max gradient norm. Duh num_train_epochs - Total number of training epochs to perform. max_steps - If > 0: set total number of training steps to perform. Override num_train_epochs . warmup_steps - Linear warmup over warmup_steps. logging_steps - Number of steps until logging occurs. save_steps - Number of steps until checkpoint is saved in output_dir save_total_limit - Limit the total amount of checkpoints, delete the older checkpoints in the output_dir , does not delete by default use_tensorboard - Only useable if tensorboard is installed return - None evaluate ( self , output_dir , per_gpu_eval_batch_size=4 , prefix='' ) output_dir - The output directory where the model predictions and checkpoints will be written. per_gpu_eval_batch_size - Batch size per GPU/CPU for evaluation. prefix - Prefix of checkpoint i.e. \"checkpoint-50\" return - Results in a dictionary evaluate_all_checkpoints ( self , output_dir , per_gpu_eval_batch_size=4 ) output_dir - The output directory where the model predictions and checkpoints will be written. per_gpu_eval_batch_size - Batch size per GPU/CPU for evaluation. return - Results in a dictionary freeze ( self ) Freeze last classification layer group only unfreeze ( self ) Unfreeze all layers freeze_to ( self , n ) Freeze first n layers of model n - Starting from initial layer, freeze all layers up to nth layer inclusively","title":"Language Model Fine-tuning"},{"location":"class-api/language-model-finetuner-module.html#lmfinetuner","text":"class adaptnlp. LMFineTuner ( train_data_file , eval_data_file=None , model_type='bert' , model_name_or_path=None , mlm=True , mlm_probability=0.15 , config_name=None , tokenizer_name=None , cache_dir=None , block_size=-1 , no_cuda=False , overwrite_cache=False , seed=42 , fp16=False , fp16_opt_level='O1' , local_rank=-1 ) A Language Model Fine Tuner object you can set language model configurations and then train and evaluate Usage: >>> finetuner = adaptnlp . LMFineTuner () >>> finetuner . train () Parameters: train_data_file - The input training data file (a text file). eval_data_file - An optional input evaluation data file to evaluate the perplexity on (a text file). model_type - The model architecture to be trained or fine-tuned. model_name_or_path - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch. mlm - Train with masked-language modeling loss instead of language modeling. mlm_probability - Ratio of tokens to mask for masked language modeling loss config_name - Optional Transformers pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config. tokenizer_name - Optional Transformers pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer. cache_dir - Optional directory to store the pre-trained models downloaded from s3 (If None, will go to default dir) block_size - Optional input sequence length after tokenization. The training dataset will be truncated in block of this size for training.\" -1 will default to the model max input length for single sentence inputs (take into account special tokens). no_cuda - Avoid using CUDA when available overwrite_cache - Overwrite the cached training and evaluation sets seed - random seed for initialization fp16 - Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit fp16_opt_level - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. local_rank - For distributed training: local_rank find_learning_rate ( self , output_dir , file_name='learning_rate.tsv' , start_learning_rate=1e-07 , end_learning_rate=10 , iterations=100 , mini_batch_size=8 , stop_early=True , smoothing_factor=0.7 , adam_epsilon=1e-08 , weight_decay=0.0 , **kwargs ) Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot This method returns a suggested learning rate using the static method LMFineTuner.suggest_learning_rate() which is implicitly run in this method. output_dir - Path to dir for learning rate file to be saved file_name - Name of learning rate .tsv file start_learning_rate - Initial learning rate to start cyclical learning rate finder method end_learning_rate - End learning rate to stop exponential increase of the learning rate iterations - Number of optimizer iterations for the ExpAnnealLR scheduler mini_batch_size - Batch size for dataloader stop_early - Bool for stopping early once loss diverges smoothing_factor - Smoothing factor on moving average of losses adam_epsilon - Epsilon for Adam optimizer. weight_decay - Weight decay if we apply some. kwargs - Additional keyword arguments for the Adam optimizer return - Learning rate as a float suggest_learning_rate ( losses , lrs , lr_diff=30 , loss_threshold=0.05 , adjust_value=1 ) Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method losses - Numpy array of losses lrs - Numpy array of exponentially increasing learning rates (must match dim of losses ) lr_diff - Learning rate Interval of slide ruler loss_threshold - Threshold of loss difference on interval where the sliding stops adjust_value - Coefficient for adjustment return - the optimal learning rate as a float train_one_cycle ( self , output_dir , should_continue=False , overwrite_output_dir=False , evaluate_during_training=False , per_gpu_train_batch_size=4 , gradient_accumulation_steps=1 , learning_rate=5e-05 , weight_decay=0.0 , adam_epsilon=1e-08 , max_grad_norm=1.0 , num_train_epochs=1.0 , max_steps=-1 , warmup_steps=0 , logging_steps=50 , save_steps=50 , save_total_limit=3 , use_tensorboard=False ) output_dir - The output directory where the model predictions and checkpoints will be written. should_continue - Whether to continue training from latest checkpoint in output_dir overwrite_output_dir - Overwrite the content of output directory output_dir evaluate_during_training - Run evaluation during training at each logging_step . per_gpu_train_batch_size - Batch size per GPU/CPU for training. (If evaluate_during_training is True, this is also the eval batch size gradient_accumulation_steps - Number of updates steps to accumulate before performing a backward/update pass learning_rate - The initial learning rate for Adam optimizer. weight_decay - Weight decay if we apply some. adam_epsilon - Epsilon for Adam optimizer. max_grad_norm - Max gradient norm. Duh num_train_epochs - Total number of training epochs to perform. max_steps - If > 0: set total number of training steps to perform. Override num_train_epochs . warmup_steps - Linear warmup over warmup_steps. logging_steps - Number of steps until logging occurs. save_steps - Number of steps until checkpoint is saved in output_dir save_total_limit - Limit the total amount of checkpoints, delete the older checkpoints in the output_dir , does not delete by default use_tensorboard - Only useable if tensorboard is installed return - None train ( self , output_dir , should_continue=False , overwrite_output_dir=False , evaluate_during_training=False , per_gpu_train_batch_size=4 , gradient_accumulation_steps=1 , learning_rate=5e-05 , weight_decay=0.0 , adam_epsilon=1e-08 , max_grad_norm=1.0 , num_train_epochs=1.0 , max_steps=-1 , warmup_steps=0 , logging_steps=50 , save_steps=50 , save_total_limit=3 , use_tensorboard=False ) output_dir - The output directory where the model predictions and checkpoints will be written. should_continue - Whether to continue training from latest checkpoint in output_dir overwrite_output_dir - Overwrite the content of output directory output_dir evaluate_during_training - Run evaluation during training at each logging_step . per_gpu_train_batch_size - Batch size per GPU/CPU for training. (If evaluate_during_training is True, this is also the eval batch size gradient_accumulation_steps - Number of updates steps to accumulate before performing a backward/update pass learning_rate - The initial learning rate for Adam optimizer. weight_decay - Weight decay if we apply some. adam_epsilon - Epsilon for Adam optimizer. max_grad_norm - Max gradient norm. Duh num_train_epochs - Total number of training epochs to perform. max_steps - If > 0: set total number of training steps to perform. Override num_train_epochs . warmup_steps - Linear warmup over warmup_steps. logging_steps - Number of steps until logging occurs. save_steps - Number of steps until checkpoint is saved in output_dir save_total_limit - Limit the total amount of checkpoints, delete the older checkpoints in the output_dir , does not delete by default use_tensorboard - Only useable if tensorboard is installed return - None evaluate ( self , output_dir , per_gpu_eval_batch_size=4 , prefix='' ) output_dir - The output directory where the model predictions and checkpoints will be written. per_gpu_eval_batch_size - Batch size per GPU/CPU for evaluation. prefix - Prefix of checkpoint i.e. \"checkpoint-50\" return - Results in a dictionary evaluate_all_checkpoints ( self , output_dir , per_gpu_eval_batch_size=4 ) output_dir - The output directory where the model predictions and checkpoints will be written. per_gpu_eval_batch_size - Batch size per GPU/CPU for evaluation. return - Results in a dictionary freeze ( self ) Freeze last classification layer group only unfreeze ( self ) Unfreeze all layers freeze_to ( self , n ) Freeze first n layers of model n - Starting from initial layer, freeze all layers up to nth layer inclusively","title":"LMFineTuner"},{"location":"class-api/question-answering-module.html","text":"EasyQuestionAnswering \u00b6 class adaptnlp. EasyQuestionAnswering ( ) Question Answering Module Usage: >>> qa = adaptnlp . EasyQuestionAnswering () >>> qa . predict_qa ( query = \"What is life?\" , context = \"Life is NLP.\" , n_best_size = 5 , mini_batch_size = 1 ) predict_qa ( self , query , context , n_best_size=5 , mini_batch_size=32 , model_name_or_path='bert-large-uncased-whole-word-masking-finetuned-squad' , **kwargs ) Predicts top_n answer spans of query in regards to context query - String or list of strings that specify the ordered questions corresponding to context context - String or list of strings that specify the ordered contexts corresponding to query n_best_size - The top n answers returned mini_batch_size - Mini batch size for inference model_name_or_path - Path to QA model or name of QA model at huggingface.co/models kwargs (Optional) - Keyword arguments for AdaptiveModel s like TransformersQuestionAnswering return - Either a list of string answers or a dict of the results TransformersQuestionAnswering \u00b6 class adaptnlp. TransformersQuestionAnswering ( tokenizer , model ) Adaptive Model for Transformers Question Answering Model Parameters tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers * model - A transformer Question Answering model load ( model_name_or_path ) Class method for loading and constructing this model model_name_or_path - A key string of one of Transformer's pre-trained Question Answering (SQUAD) models predict ( self , query , context , n_best_size=5 , mini_batch_size=32 , max_answer_length=10 , do_lower_case=False , version_2_with_negative=False , verbose_logging=False , null_score_diff_threshold=0.0 , max_seq_length=512 , doc_stride=128 , max_query_length=64 , **kwargs ) Predict method for running inference using the pre-trained question answering model query - String or list of strings that specify the ordered questions corresponding to context context - String or list of strings that specify the ordered contexts corresponding to query n_best_size - Number of top n results you want mini_batch_size - Mini batch size max_answer_length - Maximum token length for answers that are returned do_lower_case - Set as True if using uncased QA models version_2_with_negative - Set as True if using QA model with SQUAD2.0 verbose_logging - Set True if you want prediction verbose loggings null_score_diff_threshold - Threshold for predicting null(no answer) in Squad 2.0 Model. Default is 0.0. Raise this if you want fewer null answers max_seq_length - Maximum context token length. Check model configs to see max sequence length the model was trained with doc_stride - Number of token strides to take when splitting up conext into chunks of size max_seq_length max_query_length - Maximum token length for queries **kwargs (Optional) - Optional arguments for the Transformers model (mostly for saving evaluations)","title":"Question Answering"},{"location":"class-api/question-answering-module.html#easyquestionanswering","text":"class adaptnlp. EasyQuestionAnswering ( ) Question Answering Module Usage: >>> qa = adaptnlp . EasyQuestionAnswering () >>> qa . predict_qa ( query = \"What is life?\" , context = \"Life is NLP.\" , n_best_size = 5 , mini_batch_size = 1 ) predict_qa ( self , query , context , n_best_size=5 , mini_batch_size=32 , model_name_or_path='bert-large-uncased-whole-word-masking-finetuned-squad' , **kwargs ) Predicts top_n answer spans of query in regards to context query - String or list of strings that specify the ordered questions corresponding to context context - String or list of strings that specify the ordered contexts corresponding to query n_best_size - The top n answers returned mini_batch_size - Mini batch size for inference model_name_or_path - Path to QA model or name of QA model at huggingface.co/models kwargs (Optional) - Keyword arguments for AdaptiveModel s like TransformersQuestionAnswering return - Either a list of string answers or a dict of the results","title":"EasyQuestionAnswering"},{"location":"class-api/question-answering-module.html#transformersquestionanswering","text":"class adaptnlp. TransformersQuestionAnswering ( tokenizer , model ) Adaptive Model for Transformers Question Answering Model Parameters tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers * model - A transformer Question Answering model load ( model_name_or_path ) Class method for loading and constructing this model model_name_or_path - A key string of one of Transformer's pre-trained Question Answering (SQUAD) models predict ( self , query , context , n_best_size=5 , mini_batch_size=32 , max_answer_length=10 , do_lower_case=False , version_2_with_negative=False , verbose_logging=False , null_score_diff_threshold=0.0 , max_seq_length=512 , doc_stride=128 , max_query_length=64 , **kwargs ) Predict method for running inference using the pre-trained question answering model query - String or list of strings that specify the ordered questions corresponding to context context - String or list of strings that specify the ordered contexts corresponding to query n_best_size - Number of top n results you want mini_batch_size - Mini batch size max_answer_length - Maximum token length for answers that are returned do_lower_case - Set as True if using uncased QA models version_2_with_negative - Set as True if using QA model with SQUAD2.0 verbose_logging - Set True if you want prediction verbose loggings null_score_diff_threshold - Threshold for predicting null(no answer) in Squad 2.0 Model. Default is 0.0. Raise this if you want fewer null answers max_seq_length - Maximum context token length. Check model configs to see max sequence length the model was trained with doc_stride - Number of token strides to take when splitting up conext into chunks of size max_seq_length max_query_length - Maximum token length for queries **kwargs (Optional) - Optional arguments for the Transformers model (mostly for saving evaluations)","title":"TransformersQuestionAnswering"},{"location":"class-api/sequence-classifier-module.html","text":"EasySequenceClassifier \u00b6 class adaptnlp. EasySequenceClassifier ( ) Sequence classification models Usage: >>> classifier = EasySequenceClassifier () >>> classifier . tag_text ( text = \"text you want to label\" , model_name_or_path = \"en-sentiment\" ) tag_text ( self , text , model_name_or_path='en-sentiment' , mini_batch_size=32 , **kwargs ) Tags a text sequence with labels the sequence classification models have been trained on text - String, list of strings, Sentence , or list of Sentence s to be classified model_name_or_path - The model name key or model path mini_batch_size - The mini batch size for running inference **kwargs - (Optional) Keyword Arguments for Flair's TextClassifier.predict() method params return A list of Flair's Sentence 's tag_all ( self , text , mini_batch_size=32 , **kwargs ) Tags text with all labels from all sequence classification models text - Text input, it can be a string or any of Flair's Sentence input formats mini_batch_size - The mini batch size for running inference **kwargs - (Optional) Keyword Arguments for Flair's TextClassifier.predict() method params return - A list of Flair's Sentence 's FlairSequenceClassifier \u00b6 class adaptnlp. FlairSequenceClassifier ( model_name_or_path ) Adaptive Model for Flair's Sequence Classifier...very basic Usage: >>> classifier = FlairSequenceClassifier . load ( \"en-sentiment\" ) >>> classifier . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: model_name_or_path - A key string of one of Flair's pre-trained Sequence Classifier Model load ( model_name_or_path ) Class method for loading a constructing this classifier model_name_or_path - A key string of one of Flair's pre-trained Sequence Classifier Model predict ( self , text , mini_batch_size=32 , use_tokenizer=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size **kwargs (Optional) - Optional arguments for the Flair classifier TransformersSequenceClassifier \u00b6 class adaptnlp. TransformersSequenceClassifier ( tokenizer , model ) Adaptive model for Transformer's Sequence Classification Model Usage: >>> classifier = TransformersSequenceClassifier . load ( \"transformers-sc-model\" ) >>> classifier . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Sequence Classsifciation model load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained Sequence Classifier Model predict ( self , text , mini_batch_size=32 , use_tokenizer=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size **kwargs (Optional) - Optional arguments for the Transformers classifier","title":"Sequence Classifier"},{"location":"class-api/sequence-classifier-module.html#easysequenceclassifier","text":"class adaptnlp. EasySequenceClassifier ( ) Sequence classification models Usage: >>> classifier = EasySequenceClassifier () >>> classifier . tag_text ( text = \"text you want to label\" , model_name_or_path = \"en-sentiment\" ) tag_text ( self , text , model_name_or_path='en-sentiment' , mini_batch_size=32 , **kwargs ) Tags a text sequence with labels the sequence classification models have been trained on text - String, list of strings, Sentence , or list of Sentence s to be classified model_name_or_path - The model name key or model path mini_batch_size - The mini batch size for running inference **kwargs - (Optional) Keyword Arguments for Flair's TextClassifier.predict() method params return A list of Flair's Sentence 's tag_all ( self , text , mini_batch_size=32 , **kwargs ) Tags text with all labels from all sequence classification models text - Text input, it can be a string or any of Flair's Sentence input formats mini_batch_size - The mini batch size for running inference **kwargs - (Optional) Keyword Arguments for Flair's TextClassifier.predict() method params return - A list of Flair's Sentence 's","title":"EasySequenceClassifier"},{"location":"class-api/sequence-classifier-module.html#flairsequenceclassifier","text":"class adaptnlp. FlairSequenceClassifier ( model_name_or_path ) Adaptive Model for Flair's Sequence Classifier...very basic Usage: >>> classifier = FlairSequenceClassifier . load ( \"en-sentiment\" ) >>> classifier . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: model_name_or_path - A key string of one of Flair's pre-trained Sequence Classifier Model load ( model_name_or_path ) Class method for loading a constructing this classifier model_name_or_path - A key string of one of Flair's pre-trained Sequence Classifier Model predict ( self , text , mini_batch_size=32 , use_tokenizer=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size **kwargs (Optional) - Optional arguments for the Flair classifier","title":"FlairSequenceClassifier"},{"location":"class-api/sequence-classifier-module.html#transformerssequenceclassifier","text":"class adaptnlp. TransformersSequenceClassifier ( tokenizer , model ) Adaptive model for Transformer's Sequence Classification Model Usage: >>> classifier = TransformersSequenceClassifier . load ( \"transformers-sc-model\" ) >>> classifier . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Sequence Classsifciation model load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained Sequence Classifier Model predict ( self , text , mini_batch_size=32 , use_tokenizer=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size **kwargs (Optional) - Optional arguments for the Transformers classifier","title":"TransformersSequenceClassifier"},{"location":"class-api/sequence-classifier-trainer-module.html","text":"SequenceClassifierTrainer \u00b6 class adaptnlp. SequenceClassifierTrainer ( corpus , encoder , column_name_map , corpus_in_memory=True , predictive_head='flair' , **kwargs ) Sequence Classifier Trainer Usage: >>> sc_trainer = SequenceClassifierTrainer ( corpus = \"/Path/to/data/dir\" ) Parameters: corpus - A flair corpus data model or Path /string to a directory with train.csv/test.csv/dev.csv encoder - A EasyDocumentEmbeddings object if training with a flair prediction head or Path /string if training with Transformer's prediction models column_name_map - Required if corpus is not a Corpus object, it's a dictionary specifying the indices of the text and label columns of the csv i.e. {1:\"text\",2:\"label\"} corpus_in_memory - Boolean for whether to store corpus embeddings in memory predictive_head - For now either \"flair\" or \"transformers\" for the prediction head **kwargs - Keyword arguments for Flair's TextClassifier model class find_learning_rate ( self , output_dir , file_name='learning_rate.tsv' , start_learning_rate=1e-08 , end_learning_rate=10 , iterations=100 , mini_batch_size=32 , stop_early=True , smoothing_factor=0.7 , plot_learning_rate=True , **kwargs ) Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot This method returns a suggested learning rate using the static method LMFineTuner.suggest_learning_rate() which is implicitly run in this method. output_dir - Path to dir for learning rate file to be saved file_name - Name of learning rate .tsv file start_learning_rate - Initial learning rate to start cyclical learning rate finder method end_learning_rate - End learning rate to stop exponential increase of the learning rate iterations - Number of optimizer iterations for the ExpAnnealLR scheduler mini_batch_size - Batch size for dataloader stop_early - Bool for stopping early once loss diverges smoothing_factor - Smoothing factor on moving average of losses adam_epsilon - Epsilon for Adam optimizer. weight_decay - Weight decay if we apply some. kwargs - Additional keyword arguments for the Adam optimizer return - Learning rate as a float suggested_learning_rate ( losses , lrs , lr_diff=15 , loss_threshold=0.2 , adjust_value=1 ) Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method losses - Numpy array of losses lrs - Numpy array of exponentially increasing learning rates (must match dim of losses ) lr_diff - Learning rate Interval of slide ruler loss_threshold - Threshold of loss difference on interval where the sliding stops adjust_value - Coefficient for adjustment return - the optimal learning rate as a float train ( self , output_dir , learning_rate=0.07 , mini_batch_size=32 , anneal_factor=0.5 , patience=5 , max_epochs=150 , plot_weights=False , **kwargs ) Train the Sequence Classifier output_dir - The output directory where the model predictions and checkpoints will be written. learning_rate - The initial learning rate mini_batch_size - Batch size for the dataloader anneal_factor - The factor by which the learning rate is annealed patience - Patience is the number of epochs with no improvement the Trainer waits until annealing the learning rate max_epochs - Maximum number of epochs to train. Terminates training if this number is surpassed. plot_weights - Bool to plot weights or not kwargs - Keyword arguments for the rest of Flair's Trainer.train() hyperparameters","title":"Sequence Classifier Trainer"},{"location":"class-api/sequence-classifier-trainer-module.html#sequenceclassifiertrainer","text":"class adaptnlp. SequenceClassifierTrainer ( corpus , encoder , column_name_map , corpus_in_memory=True , predictive_head='flair' , **kwargs ) Sequence Classifier Trainer Usage: >>> sc_trainer = SequenceClassifierTrainer ( corpus = \"/Path/to/data/dir\" ) Parameters: corpus - A flair corpus data model or Path /string to a directory with train.csv/test.csv/dev.csv encoder - A EasyDocumentEmbeddings object if training with a flair prediction head or Path /string if training with Transformer's prediction models column_name_map - Required if corpus is not a Corpus object, it's a dictionary specifying the indices of the text and label columns of the csv i.e. {1:\"text\",2:\"label\"} corpus_in_memory - Boolean for whether to store corpus embeddings in memory predictive_head - For now either \"flair\" or \"transformers\" for the prediction head **kwargs - Keyword arguments for Flair's TextClassifier model class find_learning_rate ( self , output_dir , file_name='learning_rate.tsv' , start_learning_rate=1e-08 , end_learning_rate=10 , iterations=100 , mini_batch_size=32 , stop_early=True , smoothing_factor=0.7 , plot_learning_rate=True , **kwargs ) Uses Leslie's cyclical learning rate finding method to generate and save the loss x learning rate plot This method returns a suggested learning rate using the static method LMFineTuner.suggest_learning_rate() which is implicitly run in this method. output_dir - Path to dir for learning rate file to be saved file_name - Name of learning rate .tsv file start_learning_rate - Initial learning rate to start cyclical learning rate finder method end_learning_rate - End learning rate to stop exponential increase of the learning rate iterations - Number of optimizer iterations for the ExpAnnealLR scheduler mini_batch_size - Batch size for dataloader stop_early - Bool for stopping early once loss diverges smoothing_factor - Smoothing factor on moving average of losses adam_epsilon - Epsilon for Adam optimizer. weight_decay - Weight decay if we apply some. kwargs - Additional keyword arguments for the Adam optimizer return - Learning rate as a float suggested_learning_rate ( losses , lrs , lr_diff=15 , loss_threshold=0.2 , adjust_value=1 ) Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method losses - Numpy array of losses lrs - Numpy array of exponentially increasing learning rates (must match dim of losses ) lr_diff - Learning rate Interval of slide ruler loss_threshold - Threshold of loss difference on interval where the sliding stops adjust_value - Coefficient for adjustment return - the optimal learning rate as a float train ( self , output_dir , learning_rate=0.07 , mini_batch_size=32 , anneal_factor=0.5 , patience=5 , max_epochs=150 , plot_weights=False , **kwargs ) Train the Sequence Classifier output_dir - The output directory where the model predictions and checkpoints will be written. learning_rate - The initial learning rate mini_batch_size - Batch size for the dataloader anneal_factor - The factor by which the learning rate is annealed patience - Patience is the number of epochs with no improvement the Trainer waits until annealing the learning rate max_epochs - Maximum number of epochs to train. Terminates training if this number is surpassed. plot_weights - Bool to plot weights or not kwargs - Keyword arguments for the rest of Flair's Trainer.train() hyperparameters","title":"SequenceClassifierTrainer"},{"location":"class-api/summarizer-module.html","text":"EasySummarizer \u00b6 class adaptnlp. EasySummarizer ( ) Summarization Module Usage: >>> summarizer = EasySummarizer () >>> summarizer . summarize ( text = \"Summarize this text\" , model_name_or_path = \"t5-small\" ) summarize ( self , text , model_name_or_path='t5-small' , mini_batch_size=32 , num_beams=4 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 4. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method TransformersSummarizer \u00b6 class adaptnlp. TransformersSummarizer ( tokenizer , model ) Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart conditiional generation models have a language modeling head) Usage: >>> summarizer = TransformersSummarizer . load ( \"transformers-summarizer-model\" ) >>> summarizer . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Conditional Generation (Bart or T5) or Language model load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained Summarizer Model predict ( self , text , mini_batch_size=32 , num_beams=4 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 4. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method","title":"Summarizer"},{"location":"class-api/summarizer-module.html#easysummarizer","text":"class adaptnlp. EasySummarizer ( ) Summarization Module Usage: >>> summarizer = EasySummarizer () >>> summarizer . summarize ( text = \"Summarize this text\" , model_name_or_path = \"t5-small\" ) summarize ( self , text , model_name_or_path='t5-small' , mini_batch_size=32 , num_beams=4 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 4. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method","title":"EasySummarizer"},{"location":"class-api/summarizer-module.html#transformerssummarizer","text":"class adaptnlp. TransformersSummarizer ( tokenizer , model ) Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart conditiional generation models have a language modeling head) Usage: >>> summarizer = TransformersSummarizer . load ( \"transformers-summarizer-model\" ) >>> summarizer . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Conditional Generation (Bart or T5) or Language model load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained Summarizer Model predict ( self , text , mini_batch_size=32 , num_beams=4 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 4. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method","title":"TransformersSummarizer"},{"location":"class-api/token-tagger-module.html","text":"EasyTokenTagger \u00b6 class adaptnlp. EasyTokenTagger ( ) Token level classification models Usage: >>> tagger = adaptnlp . EasyTokenTagger () >>> tagger . tag_text ( text = \"text you want to tag\" , model_name_or_path = \"ner-ontonotes\" ) tag_text ( self , text , model_name_or_path='ner-ontonotes' , mini_batch_size=32 , **kwargs ) Tags tokens with labels the token classification models have been trained on text - Text input, it can be a string or any of Flair's Sentence input formats model_name_or_path - The hosted model name key or model path mini_batch_size - The mini batch size for running inference **kwargs - Keyword arguments for Flair's SequenceTagger.predict() method return - A list of Flair's Sentence 's tag_all ( self , text , mini_batch_size=32 , **kwargs ) Tags tokens with all labels from all token classification models text - Text input, it can be a string or any of Flair's Sentence input formats mini_batch_size - The mini batch size for running inference **kwargs - Keyword arguments for Flair's SequenceTagger.predict() method return A list of Flair's Sentence 's","title":"Token Tagger"},{"location":"class-api/token-tagger-module.html#easytokentagger","text":"class adaptnlp. EasyTokenTagger ( ) Token level classification models Usage: >>> tagger = adaptnlp . EasyTokenTagger () >>> tagger . tag_text ( text = \"text you want to tag\" , model_name_or_path = \"ner-ontonotes\" ) tag_text ( self , text , model_name_or_path='ner-ontonotes' , mini_batch_size=32 , **kwargs ) Tags tokens with labels the token classification models have been trained on text - Text input, it can be a string or any of Flair's Sentence input formats model_name_or_path - The hosted model name key or model path mini_batch_size - The mini batch size for running inference **kwargs - Keyword arguments for Flair's SequenceTagger.predict() method return - A list of Flair's Sentence 's tag_all ( self , text , mini_batch_size=32 , **kwargs ) Tags tokens with all labels from all token classification models text - Text input, it can be a string or any of Flair's Sentence input formats mini_batch_size - The mini batch size for running inference **kwargs - Keyword arguments for Flair's SequenceTagger.predict() method return A list of Flair's Sentence 's","title":"EasyTokenTagger"},{"location":"class-api/translator-module.html","text":"EasyTranslator \u00b6 class adaptnlp. EasyTranslator ( ) Translation Module Usage: >>> translator = EasyTranslator () >>> translator . translate ( text = \"translate this text\" , model_name_or_path = \"t5-small\" ) translate ( self , text , model_name_or_path='t5-small' , t5_prefix='translate English to German' , mini_batch_size=32 , num_beams=1 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size t5_prefix (Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models. num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method TransformersTranslator \u00b6 class adaptnlp. TransformersTranslator ( tokenizer , model ) Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart conditional generation models have a language modeling head) Usage: >>> translator = TransformersTranslator . load ( \"transformers-translator-model\" ) >>> translator . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Conditional Generation (Bart or T5) or Language model load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained translator Model predict ( self , text , t5_prefix='translate English to German' , mini_batch_size=32 , num_beams=1 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on t5_prefix (Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models. mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method","title":"Translator"},{"location":"class-api/translator-module.html#easytranslator","text":"class adaptnlp. EasyTranslator ( ) Translation Module Usage: >>> translator = EasyTranslator () >>> translator . translate ( text = \"translate this text\" , model_name_or_path = \"t5-small\" ) translate ( self , text , model_name_or_path='t5-small' , t5_prefix='translate English to German' , mini_batch_size=32 , num_beams=1 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on mini_batch_size - Mini batch size t5_prefix (Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models. num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method","title":"EasyTranslator"},{"location":"class-api/translator-module.html#transformerstranslator","text":"class adaptnlp. TransformersTranslator ( tokenizer , model ) Adaptive model for Transformer's Conditional Generation or Language Models (Transformer's T5 and Bart conditional generation models have a language modeling head) Usage: >>> translator = TransformersTranslator . load ( \"transformers-translator-model\" ) >>> translator . predict ( text = \"Example text\" , mini_batch_size = 32 ) Parameters: tokenizer - A tokenizer object from Huggingface's transformers (TODO)and tokenizers model - A transformers Conditional Generation (Bart or T5) or Language model load ( model_name_or_path ) Class method for loading and constructing this classifier model_name_or_path - A key string of one of Transformer's pre-trained translator Model predict ( self , text , t5_prefix='translate English to German' , mini_batch_size=32 , num_beams=1 , min_length=0 , max_length=128 , early_stopping=True , **kwargs ) Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments for parameters of the method Transformers.PreTrainedModel.generate() can be used as well. text - String, list of strings, sentences, or list of sentences to run inference on t5_prefix (Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models. mini_batch_size - Mini batch size num_beams - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1. min_length - The min length of the sequence to be generated. Default to 0 max_length - The max length of the sequence to be generated. Between min_length and infinity. Default to 128 early_stopping - if set to True beam search is stopped when at least num_beams sentences finished per batch. **kwargs (Optional) - Optional arguments for the Transformers PreTrainedModel.generate() method","title":"TransformersTranslator"},{"location":"tutorial/index.html","text":"This tutorial section goes over the NLP capabilities available through AdaptNLP and how to use them. You should ideally follow the tutorials along with the provided notebooks in the tutorials directory at the top level of the AdaptNLP library. You could also run the code snippets in these tutorials straight through the python interpreter as well. Install and Setup \u00b6 You will first need to install AdaptNLP with Python 3.6+ using the following command: pip install adaptnlp AdaptNLP is largely built on top of Flair, Transformers, and PyTorch, and dependencies will be handled on install. AdaptNLP can be used with or without GPUs. AdaptNLP will automatically make use of GPU VRAM in environment with CUAD-compatible NVIDIA GPUs and NVIDIA drivers installed. GPU-less environments will run AdaptNLP modules fine as well. Overview of NLP Capabilities \u00b6 An overview of some of the AdaptNLP capabilities via. code snippets. A good way to make sure AdaptNLP is installed correctly is by running each of these code snippets. Note this may take a while if models have not already been downloaded. Named Entity Recognition with EasyTokenTagger \u00b6 from adaptnlp import EasyTokenTagger ## Example Text example_text = \"Novetta's headquarters is located in Mclean, Virginia.\" ## Load the token tagger module and tag text with the NER model tagger = EasyTokenTagger () sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner\" ) ## Output tagged token span results in Flair's Sentence object model for sentence in sentences : for entity in sentence . get_spans ( \"ner\" ): print ( entity ) English Sentiment Classifier EasySequenceClassifier \u00b6 from adaptnlp import EasySequenceClassifier ## Example Text example_text = \"Novetta is a great company that was chosen as one of top 50 great places to work!\" ## Load the sequence classifier module and classify sequence of text with the english sentiment model classifier = EasySequenceClassifier () sentences = classifier . tag_text ( text = example_text , model_name_or_path = \"en-sentiment\" ) ## Output labeled text results in Flair's Sentence object model for sentence in sentences : print ( sentence . labels ) Language Model Embeddings EasyWordEmbeddings \u00b6 from adaptnlp import EasyWordEmbeddings ## Example Text example_text = \"Albert Einstein used to work at Novetta.\" ## Load the embeddings module and embed the tokens within the text embeddings = EasyWordEmbeddings () sentences = embeddings . embed_text ( example_text , model_name_or_path = \"gpt2\" ) # Iterate through tokens in the sentence to access embeddings for sentence in sentences : for token in sentence : print ( token . get_embedding ()) Span-based Question Answering EasyQuestionAnswering \u00b6 from adaptnlp import EasyQuestionAnswering ## Example Query and Context query = \"What is the meaning of life?\" context = \"Machine Learning is the meaning of life.\" top_n = 5 ## Load the QA module and run inference on results qa = EasyQuestionAnswering () best_answer , best_n_answers = qa . predict_bert_qa ( query = query , context = context , n_best_size = top_n ) ## Output top answer as well as top 5 answers print ( best_answer ) print ( best_n_answers ) Sequence Classification Training SequenceClassifier \u00b6 from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer # Specify corpus data directory and model output directory corpus = \"Path/to/data/directory\" OUTPUT_DIR = \"Path/to/output/directory\" # Instantiate AdaptNLP easy document embeddings module, which can take in a variable number of embeddings to make `Stacked Embeddings`. # You may also use custom Transformers LM models by specifying the path the the language model doc_embeddings = EasyDocumentEmbeddings ( model_name_or_path = \"bert-base-cased\" , methods = [ \"rnn\" ]) # Instantiate Sequence Classifier Trainer by loading in the data, data column map, and embeddings as an encoder sc_trainer = SequenceClassifierTrainer ( corpus = corpus , encoder = doc_embeddings , column_name_map = { 0 : \"text\" , 1 : \"label\" }) # Find Learning Rate learning_rate = sc_trainer . find_learning_rate ( output_dir - OUTPUT_DIR ) # Train Using Flair's Sequence Classification Head sc_trainer . train ( output_dir = OUTPUT_DIR , learning_rate = learning_rate , max_epochs = 150 ) # Predict text labels with the trained model using `EasySequenceClassifier` from adaptnlp import EasySequenceClassifier example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR / \"final-model.pt\" ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels ) Transformers Language Model Fine Tuning LMFineTuner \u00b6 from adaptnlp import LMFineTuner # Specify Text Data File Paths train_data_file = \"Path/to/train.csv\" eval_data_file = \"Path/to/test.csv\" # Instantiate Finetuner with Desired Language Model finetuner = LMFineTuner ( train_data_file = train_data_file , eval_data_file = eval_data_file , model_type = \"bert\" , model_name_or_path = \"bert-base-cased\" ) finetuner . freeze () # Find Optimal Learning Rate learning_rate = finetuner . find_learning_rate ( base_path = \"Path/to/base/directory\" ) finetuner . freeze () # Train and Save Fine Tuned Language Models finetuner . train_one_cycle ( output_dir = \"Path/to/output/directory\" , learning_rate = learning_rate )","title":"Tutorial - Intro"},{"location":"tutorial/index.html#install-and-setup","text":"You will first need to install AdaptNLP with Python 3.6+ using the following command: pip install adaptnlp AdaptNLP is largely built on top of Flair, Transformers, and PyTorch, and dependencies will be handled on install. AdaptNLP can be used with or without GPUs. AdaptNLP will automatically make use of GPU VRAM in environment with CUAD-compatible NVIDIA GPUs and NVIDIA drivers installed. GPU-less environments will run AdaptNLP modules fine as well.","title":"Install and Setup"},{"location":"tutorial/index.html#overview-of-nlp-capabilities","text":"An overview of some of the AdaptNLP capabilities via. code snippets. A good way to make sure AdaptNLP is installed correctly is by running each of these code snippets. Note this may take a while if models have not already been downloaded.","title":"Overview of NLP Capabilities"},{"location":"tutorial/index.html#named-entity-recognition-with-easytokentagger","text":"from adaptnlp import EasyTokenTagger ## Example Text example_text = \"Novetta's headquarters is located in Mclean, Virginia.\" ## Load the token tagger module and tag text with the NER model tagger = EasyTokenTagger () sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner\" ) ## Output tagged token span results in Flair's Sentence object model for sentence in sentences : for entity in sentence . get_spans ( \"ner\" ): print ( entity )","title":"Named Entity Recognition with EasyTokenTagger"},{"location":"tutorial/index.html#english-sentiment-classifier-easysequenceclassifier","text":"from adaptnlp import EasySequenceClassifier ## Example Text example_text = \"Novetta is a great company that was chosen as one of top 50 great places to work!\" ## Load the sequence classifier module and classify sequence of text with the english sentiment model classifier = EasySequenceClassifier () sentences = classifier . tag_text ( text = example_text , model_name_or_path = \"en-sentiment\" ) ## Output labeled text results in Flair's Sentence object model for sentence in sentences : print ( sentence . labels )","title":"English Sentiment Classifier EasySequenceClassifier"},{"location":"tutorial/index.html#language-model-embeddings-easywordembeddings","text":"from adaptnlp import EasyWordEmbeddings ## Example Text example_text = \"Albert Einstein used to work at Novetta.\" ## Load the embeddings module and embed the tokens within the text embeddings = EasyWordEmbeddings () sentences = embeddings . embed_text ( example_text , model_name_or_path = \"gpt2\" ) # Iterate through tokens in the sentence to access embeddings for sentence in sentences : for token in sentence : print ( token . get_embedding ())","title":"Language Model Embeddings EasyWordEmbeddings"},{"location":"tutorial/index.html#span-based-question-answering-easyquestionanswering","text":"from adaptnlp import EasyQuestionAnswering ## Example Query and Context query = \"What is the meaning of life?\" context = \"Machine Learning is the meaning of life.\" top_n = 5 ## Load the QA module and run inference on results qa = EasyQuestionAnswering () best_answer , best_n_answers = qa . predict_bert_qa ( query = query , context = context , n_best_size = top_n ) ## Output top answer as well as top 5 answers print ( best_answer ) print ( best_n_answers )","title":"Span-based Question Answering EasyQuestionAnswering"},{"location":"tutorial/index.html#sequence-classification-training-sequenceclassifier","text":"from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer # Specify corpus data directory and model output directory corpus = \"Path/to/data/directory\" OUTPUT_DIR = \"Path/to/output/directory\" # Instantiate AdaptNLP easy document embeddings module, which can take in a variable number of embeddings to make `Stacked Embeddings`. # You may also use custom Transformers LM models by specifying the path the the language model doc_embeddings = EasyDocumentEmbeddings ( model_name_or_path = \"bert-base-cased\" , methods = [ \"rnn\" ]) # Instantiate Sequence Classifier Trainer by loading in the data, data column map, and embeddings as an encoder sc_trainer = SequenceClassifierTrainer ( corpus = corpus , encoder = doc_embeddings , column_name_map = { 0 : \"text\" , 1 : \"label\" }) # Find Learning Rate learning_rate = sc_trainer . find_learning_rate ( output_dir - OUTPUT_DIR ) # Train Using Flair's Sequence Classification Head sc_trainer . train ( output_dir = OUTPUT_DIR , learning_rate = learning_rate , max_epochs = 150 ) # Predict text labels with the trained model using `EasySequenceClassifier` from adaptnlp import EasySequenceClassifier example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR / \"final-model.pt\" ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels )","title":"Sequence Classification Training SequenceClassifier"},{"location":"tutorial/index.html#transformers-language-model-fine-tuning-lmfinetuner","text":"from adaptnlp import LMFineTuner # Specify Text Data File Paths train_data_file = \"Path/to/train.csv\" eval_data_file = \"Path/to/test.csv\" # Instantiate Finetuner with Desired Language Model finetuner = LMFineTuner ( train_data_file = train_data_file , eval_data_file = eval_data_file , model_type = \"bert\" , model_name_or_path = \"bert-base-cased\" ) finetuner . freeze () # Find Optimal Learning Rate learning_rate = finetuner . find_learning_rate ( base_path = \"Path/to/base/directory\" ) finetuner . freeze () # Train and Save Fine Tuned Language Models finetuner . train_one_cycle ( output_dir = \"Path/to/output/directory\" , learning_rate = learning_rate )","title":"Transformers Language Model Fine Tuning LMFineTuner"},{"location":"tutorial/advanced-index.html","text":"This advanced tutorial section goes over using AdaptNLP for training and fine-tuning your own custom NLP models to get State-of-the-Art results. You should ideally follow the tutorials along with the provided notebooks in the tutorials directory at the top level of the AdaptNLP library. You could also run the code snippets in these tutorials straight through the python interpreter as well. Install and Setup \u00b6 AdaptNLP can be used with or without GPUs. AdaptNLP will automatically make use of GPU VRAM in environment with CUDA-compatible NVIDIA GPUs and NVIDIA drivers installed. GPU-less environments will run AdaptNLP modules fine as well. You will almost always want to utilize GPUs for training and fine-tuning useful NLP models, so a CUDA-compatible NVIDIA GPU is a must. Multi-GPU environments with Apex installed can allow for distributed and/or mixed precision training. Overview of Training and Finetuning Capabilities \u00b6 Simply training a state-of-the-art sequence classification model can be done with AdaptNLP using Flair's sequence classification model and trainer with general pre-trained language models. With encoders providing accurate word representations via. models like ALBERT, GPT2, and other transformer models, we can produce accurate NLP task-related models. With the concepts of ULMFiT in mind, AdaptNLP's approach in training downstream predictive NLP models like sequence classification takes a step further than just utilizing pre-trained contextualized embeddings. We are able to effectively fine-tune state-of-the-art language models for useful NLP tasks on various domain specific data. Training a Sequence Classification with SequenceClassifierTrainer \u00b6 from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer from flair.datasets import TREC_6 # Specify directory for trainer files and model to be downloaded to OUTPUT_DIR = \"Path/to/model/output/directory\" # Load corpus and instantiate AdaptNLP's `EasyDocumentEmbeddings` with desired embeddings corpus = TREC_6 () # Or path to directory of train.csv, test.csv, dev.csv files at \"Path/to/data/directory\" doc_embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , methods = [ \"rnn\" ]) # Instantiate the trainer for Sequence Classification with the dataset, embeddings, and mapping of column index of data sc_trainer = SequenceClassifierTrainer ( corpus = corpus , encoder = doc_embeddings ) # Find optimal learning rate with automated learning rate finder learning_rate = sc_trainer . find_learning_rate ( output_dir = OUTPUT_DIR ) # Train the sequence classifier sc_trainer . train ( output_dir = OUTPUT_DIR , learning_rate = learning_rate , mini_batch_size = 32 , max_epochs = 150 ) # Now load the `EasySequenceClassifier` with the path to your trained model and run inference on your text. from adaptnlp import EasySequenceClassifier example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels ) Fine-Tuning a Transformers Language Model with LMFineTuner \u00b6 from adaptnlp import LMFineTuner # Set output directory to store fine-tuner files and models OUTPUT_DIR = \"Path/to/model/output/directory\" # Set path to train.csv and test.csv datasets, must have a column header labeled \"text\" to specify data to train language model train_data_file = \"Path/to/train.csv\" eval_data_file = \"Path/to/test.csv\" finetuner = LMFineTuner ( train_data_file = train_data_file , eval_data_file = eval_data_file , model_type = \"bert\" , model_name_or_path = \"bert-base-cased\" , mlm = True ) # Freeze layers up to the last group of classification layers finetuner . freeze () # Find optimal learning rate with automated learning rate finder learning_rate = finetuner . find_learning_rate ( base_path = OUTPUT_DIR ) finetuner . freeze () finetuner . train_one_cycle ( output_dir = OUTPUT_DIR , learning_rate = learning_rate , per_gpu_train_batch_size = 4 , num_train_epochs = 10.0 , evaluate_during_training = True , )","title":"Advanced - Intro"},{"location":"tutorial/advanced-index.html#install-and-setup","text":"AdaptNLP can be used with or without GPUs. AdaptNLP will automatically make use of GPU VRAM in environment with CUDA-compatible NVIDIA GPUs and NVIDIA drivers installed. GPU-less environments will run AdaptNLP modules fine as well. You will almost always want to utilize GPUs for training and fine-tuning useful NLP models, so a CUDA-compatible NVIDIA GPU is a must. Multi-GPU environments with Apex installed can allow for distributed and/or mixed precision training.","title":"Install and Setup"},{"location":"tutorial/advanced-index.html#overview-of-training-and-finetuning-capabilities","text":"Simply training a state-of-the-art sequence classification model can be done with AdaptNLP using Flair's sequence classification model and trainer with general pre-trained language models. With encoders providing accurate word representations via. models like ALBERT, GPT2, and other transformer models, we can produce accurate NLP task-related models. With the concepts of ULMFiT in mind, AdaptNLP's approach in training downstream predictive NLP models like sequence classification takes a step further than just utilizing pre-trained contextualized embeddings. We are able to effectively fine-tune state-of-the-art language models for useful NLP tasks on various domain specific data.","title":"Overview of Training and Finetuning Capabilities"},{"location":"tutorial/advanced-index.html#training-a-sequence-classification-with-sequenceclassifiertrainer","text":"from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer from flair.datasets import TREC_6 # Specify directory for trainer files and model to be downloaded to OUTPUT_DIR = \"Path/to/model/output/directory\" # Load corpus and instantiate AdaptNLP's `EasyDocumentEmbeddings` with desired embeddings corpus = TREC_6 () # Or path to directory of train.csv, test.csv, dev.csv files at \"Path/to/data/directory\" doc_embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , methods = [ \"rnn\" ]) # Instantiate the trainer for Sequence Classification with the dataset, embeddings, and mapping of column index of data sc_trainer = SequenceClassifierTrainer ( corpus = corpus , encoder = doc_embeddings ) # Find optimal learning rate with automated learning rate finder learning_rate = sc_trainer . find_learning_rate ( output_dir = OUTPUT_DIR ) # Train the sequence classifier sc_trainer . train ( output_dir = OUTPUT_DIR , learning_rate = learning_rate , mini_batch_size = 32 , max_epochs = 150 ) # Now load the `EasySequenceClassifier` with the path to your trained model and run inference on your text. from adaptnlp import EasySequenceClassifier example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels )","title":"Training a Sequence Classification with SequenceClassifierTrainer"},{"location":"tutorial/advanced-index.html#fine-tuning-a-transformers-language-model-with-lmfinetuner","text":"from adaptnlp import LMFineTuner # Set output directory to store fine-tuner files and models OUTPUT_DIR = \"Path/to/model/output/directory\" # Set path to train.csv and test.csv datasets, must have a column header labeled \"text\" to specify data to train language model train_data_file = \"Path/to/train.csv\" eval_data_file = \"Path/to/test.csv\" finetuner = LMFineTuner ( train_data_file = train_data_file , eval_data_file = eval_data_file , model_type = \"bert\" , model_name_or_path = \"bert-base-cased\" , mlm = True ) # Freeze layers up to the last group of classification layers finetuner . freeze () # Find optimal learning rate with automated learning rate finder learning_rate = finetuner . find_learning_rate ( base_path = OUTPUT_DIR ) finetuner . freeze () finetuner . train_one_cycle ( output_dir = OUTPUT_DIR , learning_rate = learning_rate , per_gpu_train_batch_size = 4 , num_train_epochs = 10.0 , evaluate_during_training = True , )","title":"Fine-Tuning a Transformers Language Model with LMFineTuner"},{"location":"tutorial/embeddings.html","text":"Embeddings for NLP are the vector representations of unstructured text. Examples of applications of Word Embeddings are downstream NLP task model training and similarity search. Below, we'll walk through how we can use AdaptNLP's EasyWordEmbeddings , EasyStackedEmbeddings , and EasyDocumentEmbeddings classes. Available Language Models \u00b6 Huggingface's Transformer's model key shortcut names can be found here . The key shortcut names for their public model-sharing repository are available here as of v2.2.2 of the Transformers library. Below are the available transformers model architectures for use as an encoder: Transformer Model ALBERT DistilBERT BERT CamemBERT RoBERTa GPT GPT2 XLNet TransformerXL XLM XLMRoBERTa You can also use Flair's FlairEmbeddings who's model key shortcut names are located here You can also use AllenNLP's ELMOEmbeddings who's model key shortcut names are located here Getting Started with EasyWordEmbeddings \u00b6 With EasyWordEmbeddings , you can load in a language model and produce contextual embeddings with text input. You can look at each word's embeddings which have been contextualized by their surrounding text, meaning embedding outputs will change for the same word depending on the text as a whole. Below is an example of producing word embeddings from OpenAI's GPT2 language model. from adaptnlp import EasyWordEmbeddings example_text = \"This is Albert. My last name is Einstein. I like physics and atoms.\" # Instantiate embeddings tagger embeddings = EasyWordEmbeddings () # Get GPT2 embeddings of example text... A list of flair Sentence objects are generated sentences = embeddings . embed_text ( example_text , model_name_or_path = \"gpt2\" ) # Iterate through to access the embeddings for token in sentences [ 0 ]: print ( token . get_embedding ()) break Getting Started with EasyStackedEmbeddings \u00b6 Stacked embeddings are a simple yet important concept pointed out by Flair that can help produce state-of-the-art results in downstream NLP models. It produces contextualized word embeddings like EasyWordEmbeddings , except the embeddings are the concatenation of tensors from multiple language model. Below is an example of producing stacked word embeddings from the BERT base cased and XLNet base cased language models. EasyStackedEmbeddings take in a variable number of key shortcut names to pre-trained language models. from adaptnlp import EasyStackedEmbeddings # Instantiate stacked embeddings tagger embeddings = EasyStackedEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" ) # Run the `embed_stack` method to get the stacked embeddings outlined above sentences = embeddings . embed_text ( example_text ) # Iterate through to access the embeddings for token in sentences [ 0 ]: print ( token . get_embedding ()) break Getting Started with EasyDocumentEmbeddings \u00b6 Document embeddings you can load in a variable number of language models, just like in stacked embeddings, and produce and embedding for an entire text. Unlike EasyWordEmbeddings and EasyStackedEmbeddings , EasyDocumentEmbeddings will produce one contextualized embedding for a sequence of words using the pool or RNN method provided by Flair. If you are familiar with using Flair's RNN document embeddings, you can pass in hyperparameters through the config parameter when instantiating an EasyDocumentEmbeddings object. Below is an example of producing aan embedding from the entire text using the BERT base cased and XLNet base cased language models. We also show the embeddings you get using the pool or RNN method. from adaptnlp import EasyDocumentEmbeddings # Instantiate document embedder with stacked embeddings embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" ) # Document Pool embedding...Instead of a list of flair Sentence objects, we get one Sentence object: the document text = embeddings . embed_pool ( example_text ) #get the text/document embedding text [ 0 ] . get_embedding () # Now again but with Document RNN embedding text = embeddings . embed_rnn ( example_text ) #get the text/document embedding text [ 0 ] . get_embedding ()","title":"Embeddings"},{"location":"tutorial/embeddings.html#available-language-models","text":"Huggingface's Transformer's model key shortcut names can be found here . The key shortcut names for their public model-sharing repository are available here as of v2.2.2 of the Transformers library. Below are the available transformers model architectures for use as an encoder: Transformer Model ALBERT DistilBERT BERT CamemBERT RoBERTa GPT GPT2 XLNet TransformerXL XLM XLMRoBERTa You can also use Flair's FlairEmbeddings who's model key shortcut names are located here You can also use AllenNLP's ELMOEmbeddings who's model key shortcut names are located here","title":"Available Language Models"},{"location":"tutorial/embeddings.html#getting-started-with-easywordembeddings","text":"With EasyWordEmbeddings , you can load in a language model and produce contextual embeddings with text input. You can look at each word's embeddings which have been contextualized by their surrounding text, meaning embedding outputs will change for the same word depending on the text as a whole. Below is an example of producing word embeddings from OpenAI's GPT2 language model. from adaptnlp import EasyWordEmbeddings example_text = \"This is Albert. My last name is Einstein. I like physics and atoms.\" # Instantiate embeddings tagger embeddings = EasyWordEmbeddings () # Get GPT2 embeddings of example text... A list of flair Sentence objects are generated sentences = embeddings . embed_text ( example_text , model_name_or_path = \"gpt2\" ) # Iterate through to access the embeddings for token in sentences [ 0 ]: print ( token . get_embedding ()) break","title":"Getting Started with EasyWordEmbeddings"},{"location":"tutorial/embeddings.html#getting-started-with-easystackedembeddings","text":"Stacked embeddings are a simple yet important concept pointed out by Flair that can help produce state-of-the-art results in downstream NLP models. It produces contextualized word embeddings like EasyWordEmbeddings , except the embeddings are the concatenation of tensors from multiple language model. Below is an example of producing stacked word embeddings from the BERT base cased and XLNet base cased language models. EasyStackedEmbeddings take in a variable number of key shortcut names to pre-trained language models. from adaptnlp import EasyStackedEmbeddings # Instantiate stacked embeddings tagger embeddings = EasyStackedEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" ) # Run the `embed_stack` method to get the stacked embeddings outlined above sentences = embeddings . embed_text ( example_text ) # Iterate through to access the embeddings for token in sentences [ 0 ]: print ( token . get_embedding ()) break","title":"Getting Started with EasyStackedEmbeddings"},{"location":"tutorial/embeddings.html#getting-started-with-easydocumentembeddings","text":"Document embeddings you can load in a variable number of language models, just like in stacked embeddings, and produce and embedding for an entire text. Unlike EasyWordEmbeddings and EasyStackedEmbeddings , EasyDocumentEmbeddings will produce one contextualized embedding for a sequence of words using the pool or RNN method provided by Flair. If you are familiar with using Flair's RNN document embeddings, you can pass in hyperparameters through the config parameter when instantiating an EasyDocumentEmbeddings object. Below is an example of producing aan embedding from the entire text using the BERT base cased and XLNet base cased language models. We also show the embeddings you get using the pool or RNN method. from adaptnlp import EasyDocumentEmbeddings # Instantiate document embedder with stacked embeddings embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , \"xlnet-base-cased\" ) # Document Pool embedding...Instead of a list of flair Sentence objects, we get one Sentence object: the document text = embeddings . embed_pool ( example_text ) #get the text/document embedding text [ 0 ] . get_embedding () # Now again but with Document RNN embedding text = embeddings . embed_rnn ( example_text ) #get the text/document embedding text [ 0 ] . get_embedding ()","title":"Getting Started with EasyDocumentEmbeddings"},{"location":"tutorial/fine-tuning-language-model.html","text":"Fine-tuning a language model comes in handy when data of a target task comes from a different distribution compared to the general-domain data that was used for pretraining a language model. When fine-tuning the language model on data from a target task, the general-domain pretrained model is able to converge quickly and adapt to the idiosyncrasies of the target data. This can be seen from the efforts of ULMFiT and Jeremy Howard's and Sebastian Ruder's approach on NLP transfer learning. With AdaptNLP's LMFineTuner , we can start to fine-tune state-of-the-art pretrained transformers architecture language models provided by Huggingface's Transformers library. Below are the available transformers language models for fine-tuning with LMFineTuner Transformer Model Model Type/Architecture String Key ALBERT \"albert\" DistilBERT \"distilbert\" BERT \"bert\" CamemBERT \"camembert\" RoBERTa \"roberta\" GPT \"gpt\" GPT2 \"gpt2\" You can fine-tune on any transformers language models with the above architecture in Huggingface's Transformers library. Key shortcut names are located here . The same goes for Huggingface's public model-sharing repository, which is available here as of v2.2.2 of the Transformers library. Getting Started with LMFineTuner \u00b6 You first want to specify the paths to train.csv and test.csv files that have header column labeled text . You also want to specify the output directories for your fine-tuner and model. from adaptnlp import LMFineTuner OUTPUT_DIR = \"Path/to/model/output/directory\" train_data_file = \"Path/to/train.csv\" eval_data_file = \"Path/to/test.csv\" We can then instantiate the language model fine-tuner by specifying which model type/architecture you want, and which corresponding pretrained language model you would like to fine-tune on. Here you can also specify whether the model relies on a causal or masked language modeling loss. This is also where you would specify using distributed training and/or multi-precision. We will freeze the model up to the language modeling classification group layer before fine-tuning. ft_configs = { \"train_data_file\" : train_data_file , \"eval_data_file\" : eval_data_file , \"model_type\" : \"bert\" , \"model_name_or_path\" : \"bert-base-cased\" , \"mlm\" : True , \"mlm_probability\" : 0.15 , \"config_name\" : None , \"tokenizer_name\" : None , \"cache_dir\" : None , \"block_size\" : - 1 , \"no_cuda\" : False , \"overwrite_cache\" : False , \"seed\" : 42 , \"fp16\" : False , \"fp16_opt_level\" : \"O1\" , \"local_rank\" : - 1 , } finetuner = LMFineTuner ( ** ft_configs ) finetuner . freeze () We can then find the optimal learning rate with the help of the cyclical learning rates method by Leslie Smith. Using this along with our novel approach in automatically extracting an optimal learning rate, we can streamline training without pausing to manually extract the optimal learning rate. The built-in find_learning_rate() will automatically reinitialize the parameteres and optimizer after running the cyclical learning rates method. learning_rate_finder_configs = { \"base_path\" : OUTPUT_DIR , \"file_name\" : \"learning_rate.tsv\" , \"start_learning_rate\" : 1e-7 , \"end_learning_rate\" : 10 , \"iterations\" : 100 , \"mini_batch_size\" : 8 , \"stop_early\" : True , \"smoothing_factor\" : 0.7 , \"adam_epsilon\" : 1e-8 , \"weight_decay\" : 0.0 , } learning_rate = finetuner . find_learning_rate ( ** learning_rate_finder_configs ) finetuner . freeze () We can now train and fine-tune the model like below. train_configs = { \"output_dir\" : OUTPUT_DIR , \"should_continue\" : False , \"overwrite_output_dir\" : True , \"evaluate_during_training\" : True , \"per_gpu_train_batch_size\" : 4 , \"gradient_accumulation_steps\" : 1 , \"learning_rate\" : learning_rate , \"weight_decay\" : 0.0 , \"adam_epsilon\" : 1e-8 , \"max_grad_norm\" : 1.0 , \"num_train_epochs\" : 10.0 , \"max_steps\" : - 1 , \"warmup_steps\" : 0 , \"logging_steps\" : 50 , \"save_steps\" : 50 , \"save_total_limit\" : None , \"use_tensorboard\" : False , } finetuner . train_one_cycle ( ** train_configs ) Once the language model has been fine-tuned, we can load this model into an EasyDocumentEmbeddings object by specifying the path to the fine-tuned LM. In this case, it is saved in OUTPUT_DIR . This is what we do below. Once we have the document embeddings instantiated, we can train a downstream custom sequence classifier with embeddings produced from our fine-tuned language models. from adaptnlp import EasyDocumentEmbeddings doc_embeddings = EasyDocumentEmbeddings ( OUTPUT_DIR , methods = [ \"rnn\" ]) # We can specify to load the pool or rnn","title":"Fine Tuning Language Model"},{"location":"tutorial/fine-tuning-language-model.html#getting-started-with-lmfinetuner","text":"You first want to specify the paths to train.csv and test.csv files that have header column labeled text . You also want to specify the output directories for your fine-tuner and model. from adaptnlp import LMFineTuner OUTPUT_DIR = \"Path/to/model/output/directory\" train_data_file = \"Path/to/train.csv\" eval_data_file = \"Path/to/test.csv\" We can then instantiate the language model fine-tuner by specifying which model type/architecture you want, and which corresponding pretrained language model you would like to fine-tune on. Here you can also specify whether the model relies on a causal or masked language modeling loss. This is also where you would specify using distributed training and/or multi-precision. We will freeze the model up to the language modeling classification group layer before fine-tuning. ft_configs = { \"train_data_file\" : train_data_file , \"eval_data_file\" : eval_data_file , \"model_type\" : \"bert\" , \"model_name_or_path\" : \"bert-base-cased\" , \"mlm\" : True , \"mlm_probability\" : 0.15 , \"config_name\" : None , \"tokenizer_name\" : None , \"cache_dir\" : None , \"block_size\" : - 1 , \"no_cuda\" : False , \"overwrite_cache\" : False , \"seed\" : 42 , \"fp16\" : False , \"fp16_opt_level\" : \"O1\" , \"local_rank\" : - 1 , } finetuner = LMFineTuner ( ** ft_configs ) finetuner . freeze () We can then find the optimal learning rate with the help of the cyclical learning rates method by Leslie Smith. Using this along with our novel approach in automatically extracting an optimal learning rate, we can streamline training without pausing to manually extract the optimal learning rate. The built-in find_learning_rate() will automatically reinitialize the parameteres and optimizer after running the cyclical learning rates method. learning_rate_finder_configs = { \"base_path\" : OUTPUT_DIR , \"file_name\" : \"learning_rate.tsv\" , \"start_learning_rate\" : 1e-7 , \"end_learning_rate\" : 10 , \"iterations\" : 100 , \"mini_batch_size\" : 8 , \"stop_early\" : True , \"smoothing_factor\" : 0.7 , \"adam_epsilon\" : 1e-8 , \"weight_decay\" : 0.0 , } learning_rate = finetuner . find_learning_rate ( ** learning_rate_finder_configs ) finetuner . freeze () We can now train and fine-tune the model like below. train_configs = { \"output_dir\" : OUTPUT_DIR , \"should_continue\" : False , \"overwrite_output_dir\" : True , \"evaluate_during_training\" : True , \"per_gpu_train_batch_size\" : 4 , \"gradient_accumulation_steps\" : 1 , \"learning_rate\" : learning_rate , \"weight_decay\" : 0.0 , \"adam_epsilon\" : 1e-8 , \"max_grad_norm\" : 1.0 , \"num_train_epochs\" : 10.0 , \"max_steps\" : - 1 , \"warmup_steps\" : 0 , \"logging_steps\" : 50 , \"save_steps\" : 50 , \"save_total_limit\" : None , \"use_tensorboard\" : False , } finetuner . train_one_cycle ( ** train_configs ) Once the language model has been fine-tuned, we can load this model into an EasyDocumentEmbeddings object by specifying the path to the fine-tuned LM. In this case, it is saved in OUTPUT_DIR . This is what we do below. Once we have the document embeddings instantiated, we can train a downstream custom sequence classifier with embeddings produced from our fine-tuned language models. from adaptnlp import EasyDocumentEmbeddings doc_embeddings = EasyDocumentEmbeddings ( OUTPUT_DIR , methods = [ \"rnn\" ]) # We can specify to load the pool or rnn","title":"Getting Started with LMFineTuner"},{"location":"tutorial/question-answering.html","text":"Question Answering is the NLP task of producing a legible answer from being provided two text inputs: the context and the question in regards to the context. Examples of Question Answering models are span-based models that output a start and end index that outline the relevant \"answer\" from the context provided. With these models, we can extract answers from various questions and queries regarding any unstructured text. Below, we'll walk through how we can use AdaptNLP's EasyQuestionAnswering module to extract span-based text answers from unstructured text using state-of-the-art question answering models. Getting Started with EasyQuestionAnswering \u00b6 You can use EasyQuestionAnswering to run span-based question answering models. Providing a context and query, we get an output of top n_best_size answer predictions along with token span indices and probability scores. We'll first get started by importing the EasyQuestionAnswering class from AdaptNLP. After that, we set some example text that we'll use further down and then instantiate the QA model. from adaptnlp import EasyQuestionAnswering text = \"\"\"Amazon.com, Inc.[6] (/\u02c8\u00e6m\u0259z\u0252n/), is an American multinational technology company based in Seattle, Washington that focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. It is considered one of the Big Four technology companies along with Google, Apple, and Facebook.[7][8][9] Amazon is known for its disruption of well-established industries through technological innovation and mass scale.[10][11][12] It is the world's largest e-commerce marketplace, AI assistant provider, and cloud computing platform[13] as measured by revenue and market capitalization.[14] Amazon is the largest Internet company by revenue in the world.[15] It is the second largest private employer in the United States[16] and one of the world's most valuable companies. Amazon is the second largest technology company by revenue. Amazon was founded by Jeff Bezos on July 5, 1994, in Bellevue, Washington. The company initially started as an online marketplace for books but later expanded to sell electronics, software, video games, apparel, furniture, food, toys, and jewelry. In 2015, Amazon surpassed Walmart as the most valuable retailer in the United States by market capitalization.[17] In 2017, Amazon acquired Whole Foods Market for $13.4 billion, which vastly increased Amazon's presence as a brick-and-mortar retailer.[18] In 2018, Bezos announced that its two-day delivery service, Amazon Prime, had surpassed 100 million subscribers worldwide \"\"\" qa_model = EasyQuestionAnswering () Running inference with predict_qa(query: Union[List[str], str], context: Union[List[str], str], n_best_size: int, mini_batch_size: int, model_name_or_path: str, **kwargs) \u00b6 Now that we have the question answering model instantiated, we can run inference using the built-in predict_qa method. Note You can set model_name_or_path to any of Transformer's pretrained question answering models. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxForQuestionAnswering model. Here is an example of running inference on Transformer's DistilBERT QA model fine-tuned on SQUAD: top_prediction , all_nbest_json = qa_model . predict_qa ( query = \"What does Amazon do?\" , context = text , n_best_size = 5 , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) print ( top_prediction ) print ( all_nbest_json ) We can do the same thing but now more question/context pairs! questions = [ \"What does Amazon do?\" , \"What happened July 5, 1994?\" , \"How much did Amazon acquire Whole Foods for?\" ] top_prediction , all_nbest_json = qa_model . predict_qa ( query = questions , context = [ text ] * 3 , n_best_size = 5 , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) print ( top_prediction ) print ( all_nbest_json ) Note Check out TransformersQuestionAnswering for a more in-depth look into the additional parameters you can pass into the EasyQuestionAnswering.predict_qa method. XLNET and XLM models will be supported in the near future.","title":"Question Answering"},{"location":"tutorial/question-answering.html#getting-started-with-easyquestionanswering","text":"You can use EasyQuestionAnswering to run span-based question answering models. Providing a context and query, we get an output of top n_best_size answer predictions along with token span indices and probability scores. We'll first get started by importing the EasyQuestionAnswering class from AdaptNLP. After that, we set some example text that we'll use further down and then instantiate the QA model. from adaptnlp import EasyQuestionAnswering text = \"\"\"Amazon.com, Inc.[6] (/\u02c8\u00e6m\u0259z\u0252n/), is an American multinational technology company based in Seattle, Washington that focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. It is considered one of the Big Four technology companies along with Google, Apple, and Facebook.[7][8][9] Amazon is known for its disruption of well-established industries through technological innovation and mass scale.[10][11][12] It is the world's largest e-commerce marketplace, AI assistant provider, and cloud computing platform[13] as measured by revenue and market capitalization.[14] Amazon is the largest Internet company by revenue in the world.[15] It is the second largest private employer in the United States[16] and one of the world's most valuable companies. Amazon is the second largest technology company by revenue. Amazon was founded by Jeff Bezos on July 5, 1994, in Bellevue, Washington. The company initially started as an online marketplace for books but later expanded to sell electronics, software, video games, apparel, furniture, food, toys, and jewelry. In 2015, Amazon surpassed Walmart as the most valuable retailer in the United States by market capitalization.[17] In 2017, Amazon acquired Whole Foods Market for $13.4 billion, which vastly increased Amazon's presence as a brick-and-mortar retailer.[18] In 2018, Bezos announced that its two-day delivery service, Amazon Prime, had surpassed 100 million subscribers worldwide \"\"\" qa_model = EasyQuestionAnswering ()","title":"Getting Started with EasyQuestionAnswering"},{"location":"tutorial/question-answering.html#running-inference-with-predict_qaquery-unionliststr-str-context-unionliststr-str-n_best_size-int-mini_batch_size-int-model_name_or_path-str-kwargs","text":"Now that we have the question answering model instantiated, we can run inference using the built-in predict_qa method. Note You can set model_name_or_path to any of Transformer's pretrained question answering models. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxForQuestionAnswering model. Here is an example of running inference on Transformer's DistilBERT QA model fine-tuned on SQUAD: top_prediction , all_nbest_json = qa_model . predict_qa ( query = \"What does Amazon do?\" , context = text , n_best_size = 5 , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) print ( top_prediction ) print ( all_nbest_json ) We can do the same thing but now more question/context pairs! questions = [ \"What does Amazon do?\" , \"What happened July 5, 1994?\" , \"How much did Amazon acquire Whole Foods for?\" ] top_prediction , all_nbest_json = qa_model . predict_qa ( query = questions , context = [ text ] * 3 , n_best_size = 5 , mini_batch_size = 1 , model_name_or_path = \"distilbert-base-uncased-distilled-squad\" ) print ( top_prediction ) print ( all_nbest_json ) Note Check out TransformersQuestionAnswering for a more in-depth look into the additional parameters you can pass into the EasyQuestionAnswering.predict_qa method. XLNET and XLM models will be supported in the near future.","title":"Running inference with predict_qa(query: Union[List[str], str], context: Union[List[str], str], n_best_size: int, mini_batch_size: int, model_name_or_path: str, **kwargs)"},{"location":"tutorial/sequence-classification.html","text":"Sequence Classification (or Text Classification) is the NLP task of predicting a label for a sequence of words. A sentiment classifier is an example of a sequence classification model. Below, we'll walk through how we can use AdaptNLP's EasySequenceClassification module to label unstructured text with state-of-the-art sequence classification models. Getting Started with EasySequenceClassifier \u00b6 We'll first get started by importing the EasySequenceClassifier class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the classifier. from adaptnlp import EasySequenceClassifier review_text = [ \"That wasn't very good.\" , \"I really liked it\" , \"It was really useful\" , \"It broke after I bought it.\" ] movie_sentiment_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' classifier = EasySequenceClassifier () Tagging with tag_text(text: str, model_name_or_path: str, mini_batch_size: int, **kwargs) \u00b6 Now that we have the classifier instantiated, we are ready to load in a sequence classification model and tag the text with the built-in tag_text() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments. Note You can set model_name_or_path to any of Transformers or Flair's pretrained sequence classification models. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxForSequenceClassification model. The method returns a list of Flair's Sentence objects. Here is one example using a 5-star review-based sentiment classifier that's been trained by NLP Town : # Predict sentences = classifier . tag_text ( review_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 1 ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels ) Another example is shown below with a Flair's pre-trained sentiment classifier: Note Additional keyword arguments can be passed in as parameters for Flair's token tagging predict() method i.e. mini_batch_size , embedding_storage_mode , verbose , etc. # Predict sentences = classifier . tag_text ( movie_sentiment_text , model_name_or_path = \"en-sentiment\" ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels ) All of Flair's pretrained sequence classifiers are available for loading through the model_name_or_path parameter, and they can be found here . A path to a custom trained Flair Sequence Classifier can also be passed through the model_name_or_path param. Note: You can run tag_all() with the sequence classifier, as detailed in the Token Tagging Tutorial.","title":"Sequence Classification"},{"location":"tutorial/sequence-classification.html#getting-started-with-easysequenceclassifier","text":"We'll first get started by importing the EasySequenceClassifier class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the classifier. from adaptnlp import EasySequenceClassifier review_text = [ \"That wasn't very good.\" , \"I really liked it\" , \"It was really useful\" , \"It broke after I bought it.\" ] movie_sentiment_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' classifier = EasySequenceClassifier ()","title":"Getting Started with EasySequenceClassifier"},{"location":"tutorial/sequence-classification.html#tagging-with-tag_texttext-str-model_name_or_path-str-mini_batch_size-int-kwargs","text":"Now that we have the classifier instantiated, we are ready to load in a sequence classification model and tag the text with the built-in tag_text() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments. Note You can set model_name_or_path to any of Transformers or Flair's pretrained sequence classification models. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxForSequenceClassification model. The method returns a list of Flair's Sentence objects. Here is one example using a 5-star review-based sentiment classifier that's been trained by NLP Town : # Predict sentences = classifier . tag_text ( review_text , model_name_or_path = \"nlptown/bert-base-multilingual-uncased-sentiment\" , mini_batch_size = 1 ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels ) Another example is shown below with a Flair's pre-trained sentiment classifier: Note Additional keyword arguments can be passed in as parameters for Flair's token tagging predict() method i.e. mini_batch_size , embedding_storage_mode , verbose , etc. # Predict sentences = classifier . tag_text ( movie_sentiment_text , model_name_or_path = \"en-sentiment\" ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels ) All of Flair's pretrained sequence classifiers are available for loading through the model_name_or_path parameter, and they can be found here . A path to a custom trained Flair Sequence Classifier can also be passed through the model_name_or_path param. Note: You can run tag_all() with the sequence classifier, as detailed in the Token Tagging Tutorial.","title":"Tagging with tag_text(text: str, model_name_or_path: str, mini_batch_size: int, **kwargs)"},{"location":"tutorial/summarization.html","text":"Summarization is the NLP task of compressing one or many documents but still retain the input's original context and meaning. Below, we'll walk through how we can use AdaptNLP's EasySummarizer module to summarize large amounts of text with state-of-the-art models. Getting Started with EasySummarizer \u00b6 We'll first get started by importing the EasySummarizer class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the summarizer. from adaptnlp import EasySummarizer # Text from encyclopedia Britannica on Einstein text = [ \"\"\"Einstein\u2019s education was disrupted by his father\u2019s repeated failures at business. In 1894, after his company failed to get an important contract to electrify the city of Munich, Hermann Einstein moved to Milan to work with a relative. Einstein was left at a boardinghouse in Munich and expected to finish his education. Alone, miserable, and repelled by the looming prospect of military duty when he turned 16, Einstein ran away six months later and landed on the doorstep of his surprised parents. His parents realized the enormous problems that he faced as a school dropout and draft dodger with no employable skills. His prospects did not look promising. Fortunately, Einstein could apply directly to the Eidgen\u00f6ssische Polytechnische Schule (\u201cSwiss Federal Polytechnic School\u201d; in 1911, following expansion in 1909 to full university status, it was renamed the Eidgen\u00f6ssische Technische Hochschule, or \u201cSwiss Federal Institute of Technology\u201d) in Z\u00fcrich without the equivalent of a high school diploma if he passed its stiff entrance examinations. His marks showed that he excelled in mathematics and physics, but he failed at French, chemistry, and biology. Because of his exceptional math scores, he was allowed into the polytechnic on the condition that he first finish his formal schooling. He went to a special high school run by Jost Winteler in Aarau, Switzerland, and graduated in 1896. He also renounced his German citizenship at that time. (He was stateless until 1901, when he was granted Swiss citizenship.) He became lifelong friends with the Winteler family, with whom he had been boarding. (Winteler\u2019s daughter, Marie, was Einstein\u2019s first love; Einstein\u2019s sister, Maja, would eventually marry Winteler\u2019s son Paul; and his close friend Michele Besso would marry their eldest daughter, Anna.)\"\"\" , \"\"\"Einstein would write that two \u201cwonders\u201d deeply affected his early years. The first was his encounter with a compass at age five. He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would never amount to anything.\"\"\" ] summarizer = EasySummarizer () Summarizing with `summarize(text: str, model_name_or_path: str, mini_batch_size: int, num_beams:int, min_length: \u00b6 int, max_length: int, early_stopping: bool **kwargs)` Now that we have the summarizer instantiated, we are ready to load in a model and compress the text with the built-in summarize() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Summarization Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is one example using the T5-small model: # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , num_beams = 4 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Another example is shown below using the Bart-large trained on CNN data: # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"bart-large-cnn\" , mini_batch_size = 1 , num_beams = 2 , min_length = 40 , max_length = 300 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Below are some examples of Hugging Face's Pre-Trained Summarization models that you can use (These do not include models hosted in Hugging Face's model repo): Model ID T5 't5-small' 't5-base' 't5-large' 't5-3B' 't5-11B' Bart 'bart-large-cnn'","title":"Summarization"},{"location":"tutorial/summarization.html#getting-started-with-easysummarizer","text":"We'll first get started by importing the EasySummarizer class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the summarizer. from adaptnlp import EasySummarizer # Text from encyclopedia Britannica on Einstein text = [ \"\"\"Einstein\u2019s education was disrupted by his father\u2019s repeated failures at business. In 1894, after his company failed to get an important contract to electrify the city of Munich, Hermann Einstein moved to Milan to work with a relative. Einstein was left at a boardinghouse in Munich and expected to finish his education. Alone, miserable, and repelled by the looming prospect of military duty when he turned 16, Einstein ran away six months later and landed on the doorstep of his surprised parents. His parents realized the enormous problems that he faced as a school dropout and draft dodger with no employable skills. His prospects did not look promising. Fortunately, Einstein could apply directly to the Eidgen\u00f6ssische Polytechnische Schule (\u201cSwiss Federal Polytechnic School\u201d; in 1911, following expansion in 1909 to full university status, it was renamed the Eidgen\u00f6ssische Technische Hochschule, or \u201cSwiss Federal Institute of Technology\u201d) in Z\u00fcrich without the equivalent of a high school diploma if he passed its stiff entrance examinations. His marks showed that he excelled in mathematics and physics, but he failed at French, chemistry, and biology. Because of his exceptional math scores, he was allowed into the polytechnic on the condition that he first finish his formal schooling. He went to a special high school run by Jost Winteler in Aarau, Switzerland, and graduated in 1896. He also renounced his German citizenship at that time. (He was stateless until 1901, when he was granted Swiss citizenship.) He became lifelong friends with the Winteler family, with whom he had been boarding. (Winteler\u2019s daughter, Marie, was Einstein\u2019s first love; Einstein\u2019s sister, Maja, would eventually marry Winteler\u2019s son Paul; and his close friend Michele Besso would marry their eldest daughter, Anna.)\"\"\" , \"\"\"Einstein would write that two \u201cwonders\u201d deeply affected his early years. The first was his encounter with a compass at age five. He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would never amount to anything.\"\"\" ] summarizer = EasySummarizer ()","title":"Getting Started with EasySummarizer"},{"location":"tutorial/summarization.html#summarizing-with-summarizetext-str-model_name_or_path-str-mini_batch_size-int-num_beamsint-min_length","text":"int, max_length: int, early_stopping: bool **kwargs)` Now that we have the summarizer instantiated, we are ready to load in a model and compress the text with the built-in summarize() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Summarization Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is one example using the T5-small model: # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , num_beams = 4 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Another example is shown below using the Bart-large trained on CNN data: # Summarize summaries = summarizer . summarize ( text = text , model_name_or_path = \"bart-large-cnn\" , mini_batch_size = 1 , num_beams = 2 , min_length = 40 , max_length = 300 , early_stopping = True ) print ( \"Summaries: \\n \" ) for s in summaries : print ( s , \" \\n \" ) Below are some examples of Hugging Face's Pre-Trained Summarization models that you can use (These do not include models hosted in Hugging Face's model repo): Model ID T5 't5-small' 't5-base' 't5-large' 't5-3B' 't5-11B' Bart 'bart-large-cnn'","title":"Summarizing with `summarize(text: str, model_name_or_path: str, mini_batch_size: int, num_beams:int, min_length:"},{"location":"tutorial/token-tagging.html","text":"Token tagging (or token classification) is the NLP task of assigning a label to each individual word in the provided text. Examples of token tagging models are Named Entity Recognition(NER) and Parts of Speech(POS) models. With these models, we can generate tagged entities or parts of speech from unstructured text like \"Persons\" and \"Nouns\" Below, we'll walk through how we can use AdaptNLP's EasytokenTagger module to label unstructured text with state-of-the-art token tagging models. Getting Started with EasyTokenTagger \u00b6 We'll first get started by importing the EasyTokenTagger module from AdaptNLP. After that, we set some example text and instantiate the tagger. from adaptnlp import EasyTokenTagger example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' tagger = EasyTokenTagger () Tagging with tag_text(text: str, model_name_or_path: str, **kwargs) \u00b6 Now that we have the tagger instantiated, we are ready to load in a token tagging model and tag the text with the built-in tag_text() method. This method takes in parameters: text and model_name_or_path . The method returns a list of Flair's Sentence objects. Note: Additional keyword arguments can be passed in as parameters for Flair's token tagging predict() method i.e. mini_batch_size , embedding_storage_mode , verbose , etc. # Tag the string sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner-ontonotes\" ) All of Flair's pretrained token taggers are available for loading through the model_name_or_path parameter, and they can be found here . A path to a custom trained Flair Token Tagger can also be passed through the model_name_or_path param. Flair's pretrained token taggers (taken from the link above): English Models \u00b6 ID Task Training Dataset Accuracy 'ner' 4-class Named Entity Recognition Conll-03 93.03 (F1) 'ner-ontonotes' 18-class Named Entity Recognition Ontonotes 89.06 (F1) 'chunk' Syntactic Chunking Conll-2000 96.47 (F1) 'pos' Part-of-Speech Tagging Ontonotes 98.6 (Accuracy) 'frame' Semantic Frame Detection Propbank 3.0 97.54 (F1) Faster Models for CPU use \u00b6 ID Task Training Dataset Accuracy 'ner-fast' 4-class Named Entity Recognition Conll-03 92.75 (F1) 'ner-ontonotes-fast' 18-class Named Entity Recognition Ontonotes 89.27 (F1) 'chunk-fast' Syntactic Chunking Conll-2000 96.22 (F1) 'pos-fast' Part-of-Speech Tagging Ontonotes 98.47 (Accuracy) 'frame-fast' Semantic Frame Detection Propbank 3.0 97.31 (F1) Multilingual Models \u00b6 ID Task Training Dataset Accuracy 'ner-multi' 4-class Named Entity Recognition Conll-03 (4 languages) 89.27 (average F1) 'ner-multi-fast' 4-class Named Entity Recognition Conll-03 (4 languages) 87.91 (average F1) 'ner-multi-fast-learn' 4-class Named Entity Recognition Conll-03 (4 languages) 88.18 (average F1) 'pos-multi' Part-of-Speech Tagging Universal Dependency Treebank (12 languages) 96.41 (average acc.) 'pos-multi-fast' Part-of-Speech Tagging Universal Dependency Treebank (12 languages) 92.88 (average acc.) German Models \u00b6 ID Task Training Dataset Accuracy Contributor 'de-ner' 4-class Named Entity Recognition Conll-03 87.94 (F1) 'de-ner-germeval' 4+4-class Named Entity Recognition Germeval 84.90 (F1) 'de-pos' Part-of-Speech Tagging UD German - HDT 98.33 (Accuracy) 'de-pos-fine-grained' Part-of-Speech Tagging German Tweets 93.06 (Accuracy) stefan-it Other Languages \u00b6 ID Task Training Dataset Accuracy Contributor 'fr-ner' Named Entity Recognition WikiNER (aij-wikiner-fr-wp3) 95.57 (F1) mhham 'nl-ner' Named Entity Recognition CoNLL 2002 89.56 (F1) stefan-it 'da-ner' Named Entity Recognition Danish NER dataset AmaliePauli 'da-pos' Named Entity Recognition Danish Dependency Treebank AmaliePauli Now that the text has been tagged, take the returned sentences and see your results: # See Results print ( \"List string outputs of tags: \\n \" ) for sen in sentences : print ( sen . to_tagged_string ()) If you want just the entities, you can run the below but you'll need to specify the tag_type as \"ner\" or \"pos\" etc. (more information can be found in Flair's documentation): print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( tag_type = \"ner\" ): print ( entity ) Here are some additional tag_types that support some of Flair's pre-trained token taggers: tag_type Description 'ner' For Named Entity Recognition tagged text 'pos' For Parts of Speech tagged text 'np' For Syntactic Chunking tagged text NOTE: You can add your own tag_types when running the sequence classifier trainer in AdaptNLP. Tagging with tag_all(text: str, model_name_or_path: str, **kwargs) \u00b6 As you tag text with multiple pretrained token tagging models, your tagger will have multiple models loaded...memory permitting. You can then use the built-in tag_all() method to tag your text with all models that are currently loaded in your tagger. See an example below: from adaptnlp import EasyTokenTagger example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' # Load models by tagging text tagger = EasyTokenTagger () tagger . tag_text ( text = example_text , model_name_or_path = \"ner-ontonotes\" ) tagger . tag_text ( text = example_text , model_name_or_path = \"pos\" ) # Now that the \"pos\" and \"ner-ontonotes\" models are loaded, run tag_all() sentences = tagger . tag_all ( text = example_text ) Now we can see below that you get a list of Flair sentences tagged with the \"ner-ontonotes\" AND \"pos\" model: print ( \"List entities tagged: \\n \" ) print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( tag_type = \"pos\" ): print ( entity ) for sen in sentences : for entity in sen . get_spans ( tag_type = \"ner\" ): print ( entity )","title":"Token Tagging"},{"location":"tutorial/token-tagging.html#getting-started-with-easytokentagger","text":"We'll first get started by importing the EasyTokenTagger module from AdaptNLP. After that, we set some example text and instantiate the tagger. from adaptnlp import EasyTokenTagger example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' tagger = EasyTokenTagger ()","title":"Getting Started with EasyTokenTagger"},{"location":"tutorial/token-tagging.html#tagging-with-tag_texttext-str-model_name_or_path-str-kwargs","text":"Now that we have the tagger instantiated, we are ready to load in a token tagging model and tag the text with the built-in tag_text() method. This method takes in parameters: text and model_name_or_path . The method returns a list of Flair's Sentence objects. Note: Additional keyword arguments can be passed in as parameters for Flair's token tagging predict() method i.e. mini_batch_size , embedding_storage_mode , verbose , etc. # Tag the string sentences = tagger . tag_text ( text = example_text , model_name_or_path = \"ner-ontonotes\" ) All of Flair's pretrained token taggers are available for loading through the model_name_or_path parameter, and they can be found here . A path to a custom trained Flair Token Tagger can also be passed through the model_name_or_path param. Flair's pretrained token taggers (taken from the link above):","title":"Tagging with tag_text(text: str, model_name_or_path: str, **kwargs)"},{"location":"tutorial/token-tagging.html#english-models","text":"ID Task Training Dataset Accuracy 'ner' 4-class Named Entity Recognition Conll-03 93.03 (F1) 'ner-ontonotes' 18-class Named Entity Recognition Ontonotes 89.06 (F1) 'chunk' Syntactic Chunking Conll-2000 96.47 (F1) 'pos' Part-of-Speech Tagging Ontonotes 98.6 (Accuracy) 'frame' Semantic Frame Detection Propbank 3.0 97.54 (F1)","title":"English Models"},{"location":"tutorial/token-tagging.html#faster-models-for-cpu-use","text":"ID Task Training Dataset Accuracy 'ner-fast' 4-class Named Entity Recognition Conll-03 92.75 (F1) 'ner-ontonotes-fast' 18-class Named Entity Recognition Ontonotes 89.27 (F1) 'chunk-fast' Syntactic Chunking Conll-2000 96.22 (F1) 'pos-fast' Part-of-Speech Tagging Ontonotes 98.47 (Accuracy) 'frame-fast' Semantic Frame Detection Propbank 3.0 97.31 (F1)","title":"Faster Models for CPU use"},{"location":"tutorial/token-tagging.html#multilingual-models","text":"ID Task Training Dataset Accuracy 'ner-multi' 4-class Named Entity Recognition Conll-03 (4 languages) 89.27 (average F1) 'ner-multi-fast' 4-class Named Entity Recognition Conll-03 (4 languages) 87.91 (average F1) 'ner-multi-fast-learn' 4-class Named Entity Recognition Conll-03 (4 languages) 88.18 (average F1) 'pos-multi' Part-of-Speech Tagging Universal Dependency Treebank (12 languages) 96.41 (average acc.) 'pos-multi-fast' Part-of-Speech Tagging Universal Dependency Treebank (12 languages) 92.88 (average acc.)","title":"Multilingual Models"},{"location":"tutorial/token-tagging.html#german-models","text":"ID Task Training Dataset Accuracy Contributor 'de-ner' 4-class Named Entity Recognition Conll-03 87.94 (F1) 'de-ner-germeval' 4+4-class Named Entity Recognition Germeval 84.90 (F1) 'de-pos' Part-of-Speech Tagging UD German - HDT 98.33 (Accuracy) 'de-pos-fine-grained' Part-of-Speech Tagging German Tweets 93.06 (Accuracy) stefan-it","title":"German Models"},{"location":"tutorial/token-tagging.html#other-languages","text":"ID Task Training Dataset Accuracy Contributor 'fr-ner' Named Entity Recognition WikiNER (aij-wikiner-fr-wp3) 95.57 (F1) mhham 'nl-ner' Named Entity Recognition CoNLL 2002 89.56 (F1) stefan-it 'da-ner' Named Entity Recognition Danish NER dataset AmaliePauli 'da-pos' Named Entity Recognition Danish Dependency Treebank AmaliePauli Now that the text has been tagged, take the returned sentences and see your results: # See Results print ( \"List string outputs of tags: \\n \" ) for sen in sentences : print ( sen . to_tagged_string ()) If you want just the entities, you can run the below but you'll need to specify the tag_type as \"ner\" or \"pos\" etc. (more information can be found in Flair's documentation): print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( tag_type = \"ner\" ): print ( entity ) Here are some additional tag_types that support some of Flair's pre-trained token taggers: tag_type Description 'ner' For Named Entity Recognition tagged text 'pos' For Parts of Speech tagged text 'np' For Syntactic Chunking tagged text NOTE: You can add your own tag_types when running the sequence classifier trainer in AdaptNLP.","title":"Other Languages"},{"location":"tutorial/token-tagging.html#tagging-with-tag_alltext-str-model_name_or_path-str-kwargs","text":"As you tag text with multiple pretrained token tagging models, your tagger will have multiple models loaded...memory permitting. You can then use the built-in tag_all() method to tag your text with all models that are currently loaded in your tagger. See an example below: from adaptnlp import EasyTokenTagger example_text = '''Novetta Solutions is the best. Albert Einstein used to be employed at Novetta Solutions. The Wright brothers loved to visit the JBF headquarters, and they would have a chat with Albert.''' # Load models by tagging text tagger = EasyTokenTagger () tagger . tag_text ( text = example_text , model_name_or_path = \"ner-ontonotes\" ) tagger . tag_text ( text = example_text , model_name_or_path = \"pos\" ) # Now that the \"pos\" and \"ner-ontonotes\" models are loaded, run tag_all() sentences = tagger . tag_all ( text = example_text ) Now we can see below that you get a list of Flair sentences tagged with the \"ner-ontonotes\" AND \"pos\" model: print ( \"List entities tagged: \\n \" ) print ( \"List entities tagged: \\n \" ) for sen in sentences : for entity in sen . get_spans ( tag_type = \"pos\" ): print ( entity ) for sen in sentences : for entity in sen . get_spans ( tag_type = \"ner\" ): print ( entity )","title":"Tagging with tag_all(text: str, model_name_or_path: str, **kwargs)"},{"location":"tutorial/training-sequence-classification.html","text":"A sequence classifier predicts a categorical label from the unstructured sequence of text that is provided as input. AdaptNLP's SequenceClassifierTrainer uses Flair's sequence classification prediction head with Transformer and/or Flair's contextualized embeddings. You can specify the encoder you want to use from any of the following pretrained transformer language models provided by Huggingface's Transformers library. The model key shortcut names are located here . The key shortcut names of their public model-sharing repository are available here as of v2.2.2 of the Transformers library. Below are the available transformers model architectures for use as an encoder: Transformer Model ALBERT DistilBERT BERT CamemBERT RoBERTa GPT GPT2 XLNet TransformerXL XLM XLMRoBERTa You can also use Flair's FlairEmbeddings who's model key shortcut names are located here You can also use AllenNLP's ELMOEmbeddings who's model key shortcut names are located here Getting Started with SequenceClassifierTrainer \u00b6 We want to start by specifying three things: 1. corpus : A Flair Corpus data model object that contains train, test, and dev datasets. This can also be a path to a directory that contains train.csv , test.csv , and dev.csv files. If a path to the files is provided, you will require a column_name_map parameter that maps the indices of the text and label column headers i.e. {0: \"text\", 1: \"label\"} the colummn with text being at index 0 of the csv 2. output_dir : A path to a directory to store trainer and model files 3. doc_embeddings : The EasyDocumentEmbeddings object that has the specified key shortcut names to pretrained language models that the trainer will use as its encoder. from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer from flair.datasets import TREC_6 corpus = TREC_6 () # Or path to directory of train.csv, test.csv, dev.csv files at \"Path/to/data/directory\" OUTPUT_DIR = \"Path/to/model/output/directory\" doc_embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , methods = [ \"rnn\" ]) # We can specify to load the pool or rnn # methods to avoid loading both. Then we want to instantiate the trainer with the following parameters sc_configs = { \"corpus\" : corpus , \"encoder\" : doc_embeddings , \"corpus_in_memory\" : True , } sc_trainer = SequenceClassifierTrainer ( ** sc_configs ) We can then find the optimal learning rate with the help of the cyclical learning rates method by Leslie Smith. Using this along with our novel approach in automatically extracting an optimal learning rate, we can streamline training without pausing to manually extract the optimal learning rate. The built-in find_learning_rate() will automatically reinitialize the parameteres and optimizer after running the cyclical learning rates method. sc_lr_configs = { \"output_dir\" : OUTPUT_DIR , \"start_learning_rate\" : 1e-8 , \"end_learning_rate\" : 10 , \"iterations\" : 100 , \"mini_batch_size\" : 32 , \"stop_early\" : True , \"smoothing_factor\" : 0.8 , \"plot_learning_rate\" : True , } learning_rate = sc_trainer . find_learning_rate ( ** sc_lr_configs ) We can then kick off training below. sc_train_configs = { \"output_dir\" : OUTPUT_DIR , \"learning_rate\" : learning_rate , \"mini_batch_size\" : 32 , \"anneal_factor\" : 0.5 , \"patience\" : 5 , \"max_epochs\" : 150 , \"plot_weights\" : False , \"batch_growth_annealing\" : False , } sc_trainer . train ( ** sc_train_configs ) The model was saved in the directory OUTPUT_DIR . We can load the sequence classifier into our EasySequenceClassifier instance and start running inference. from adaptnlp import EasySequenceClassifier # Set example text and instantiate tagger instance example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels )","title":"Training Sequence Classification"},{"location":"tutorial/training-sequence-classification.html#getting-started-with-sequenceclassifiertrainer","text":"We want to start by specifying three things: 1. corpus : A Flair Corpus data model object that contains train, test, and dev datasets. This can also be a path to a directory that contains train.csv , test.csv , and dev.csv files. If a path to the files is provided, you will require a column_name_map parameter that maps the indices of the text and label column headers i.e. {0: \"text\", 1: \"label\"} the colummn with text being at index 0 of the csv 2. output_dir : A path to a directory to store trainer and model files 3. doc_embeddings : The EasyDocumentEmbeddings object that has the specified key shortcut names to pretrained language models that the trainer will use as its encoder. from adaptnlp import EasyDocumentEmbeddings , SequenceClassifierTrainer from flair.datasets import TREC_6 corpus = TREC_6 () # Or path to directory of train.csv, test.csv, dev.csv files at \"Path/to/data/directory\" OUTPUT_DIR = \"Path/to/model/output/directory\" doc_embeddings = EasyDocumentEmbeddings ( \"bert-base-cased\" , methods = [ \"rnn\" ]) # We can specify to load the pool or rnn # methods to avoid loading both. Then we want to instantiate the trainer with the following parameters sc_configs = { \"corpus\" : corpus , \"encoder\" : doc_embeddings , \"corpus_in_memory\" : True , } sc_trainer = SequenceClassifierTrainer ( ** sc_configs ) We can then find the optimal learning rate with the help of the cyclical learning rates method by Leslie Smith. Using this along with our novel approach in automatically extracting an optimal learning rate, we can streamline training without pausing to manually extract the optimal learning rate. The built-in find_learning_rate() will automatically reinitialize the parameteres and optimizer after running the cyclical learning rates method. sc_lr_configs = { \"output_dir\" : OUTPUT_DIR , \"start_learning_rate\" : 1e-8 , \"end_learning_rate\" : 10 , \"iterations\" : 100 , \"mini_batch_size\" : 32 , \"stop_early\" : True , \"smoothing_factor\" : 0.8 , \"plot_learning_rate\" : True , } learning_rate = sc_trainer . find_learning_rate ( ** sc_lr_configs ) We can then kick off training below. sc_train_configs = { \"output_dir\" : OUTPUT_DIR , \"learning_rate\" : learning_rate , \"mini_batch_size\" : 32 , \"anneal_factor\" : 0.5 , \"patience\" : 5 , \"max_epochs\" : 150 , \"plot_weights\" : False , \"batch_growth_annealing\" : False , } sc_trainer . train ( ** sc_train_configs ) The model was saved in the directory OUTPUT_DIR . We can load the sequence classifier into our EasySequenceClassifier instance and start running inference. from adaptnlp import EasySequenceClassifier # Set example text and instantiate tagger instance example_text = '''Where was the Queen's wedding held? ''' classifier = EasySequenceClassifier () sentences = classifier . tag_text ( example_text , model_name_or_path = OUTPUT_DIR ) print ( \"Label output: \\n \" ) for sentence in sentences : print ( sentence . labels )","title":"Getting Started with SequenceClassifierTrainer"},{"location":"tutorial/translation.html","text":"Translation is the task of producing the input text in another language. Below, we'll walk through how we can use AdaptNLP's EasyTranslator module to translate text with state-of-the-art models. Getting Started with EasyTranslator \u00b6 We'll first get started by importing the EasyTranslator class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the translator. from adaptnlp import EasyTranslator text = [ \"Machine learning will take over the world very soon.\" , \"Machines can speak in many languages.\" ,] translator = EasyTranslator () Translating with `translate(text: str, model_name_or_path: str, mini_batch_size: int, num_beams:int, min_length: \u00b6 int, max_length: int, early_stopping: bool **kwargs)` Now that we have the translator instantiated, we are ready to load in a model and translate the text with the built-in translate() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Translation Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is an example using the T5-small model: # Translate translations = translator . translate ( text = text , t5_prefix = \"translate English to German\" , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Translations: \\n \" ) for t in translations : print ( t , \" \\n \" ) Below are some examples of Hugging Face's Pre-Trained Translation models that you can use (These do not include models hosted in Hugging Face's model repo): Model ID T5 't5-small' 't5-base' 't5-large' 't5-3B' 't5-11B'","title":"Translation"},{"location":"tutorial/translation.html#getting-started-with-easytranslator","text":"We'll first get started by importing the EasyTranslator class from AdaptNLP. After that, we set some example text that we'll use further down, and then instantiate the translator. from adaptnlp import EasyTranslator text = [ \"Machine learning will take over the world very soon.\" , \"Machines can speak in many languages.\" ,] translator = EasyTranslator ()","title":"Getting Started with EasyTranslator"},{"location":"tutorial/translation.html#translating-with-translatetext-str-model_name_or_path-str-mini_batch_size-int-num_beamsint-min_length","text":"int, max_length: int, early_stopping: bool **kwargs)` Now that we have the translator instantiated, we are ready to load in a model and translate the text with the built-in translate() method. This method takes in parameters: text , model_name_or_path , and mini_batch_size as well as optional keyword arguments from the Transformers.PreTrainedModel.generate() method. Note You can set model_name_or_path to any of Transformers pretrained Translation Models with Language Model heads. Transformers models are located at https://huggingface.co/models . You can also pass in the path of a custom trained Transformers xxxWithLMHead model. The method returns a list of Strings. Here is an example using the T5-small model: # Translate translations = translator . translate ( text = text , t5_prefix = \"translate English to German\" , model_name_or_path = \"t5-small\" , mini_batch_size = 1 , min_length = 0 , max_length = 100 , early_stopping = True ) print ( \"Translations: \\n \" ) for t in translations : print ( t , \" \\n \" ) Below are some examples of Hugging Face's Pre-Trained Translation models that you can use (These do not include models hosted in Hugging Face's model repo): Model ID T5 't5-small' 't5-base' 't5-large' 't5-3B' 't5-11B'","title":"Translating with `translate(text: str, model_name_or_path: str, mini_batch_size: int, num_beams:int, min_length:"}]}